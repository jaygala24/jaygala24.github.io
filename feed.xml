<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jaygala24.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jaygala24.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-23T02:38:14+00:00</updated><id>https://jaygala24.github.io/feed.xml</id><title type="html">blank</title><subtitle>A personal website to showcase the research and development along with the blogs. </subtitle><entry><title type="html">Long Live Transformers!</title><link href="https://jaygala24.github.io/blog/2021/long_live_transformers/" rel="alternate" type="text/html" title="Long Live Transformers!"/><published>2021-06-15T00:00:00+00:00</published><updated>2021-06-15T00:00:00+00:00</updated><id>https://jaygala24.github.io/blog/2021/long_live_transformers</id><content type="html" xml:base="https://jaygala24.github.io/blog/2021/long_live_transformers/"><![CDATA[<p>üìå Note: I would like to thank <a href="https://github.com/deep1401">Deep Gandhi</a> and <a href="https://github.com/PranjalChitale">Pranjal Chitale</a> for reviewing this blog and providing valuable feedback.</p> <p>This post assumes that you have a basic understanding of Neural Networks, specifically Recurrent Neural Networks (RNNs) and Bahdanau‚Äôs attention mechanism. If you are new to the above-mentioned concepts or you‚Äôd like to brush up, I would highly recommend reading <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> and Attentional Interfaces section from <a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a>.</p> <p>Sequence-to-sequence (Seq2Seq) models have achieved a lot of success in various tasks such as machine translation, text summarization, question answering, etc. RNNs were the primary choice for seq2seq as they are useful for learning variable-length sequential data, but their sequential nature inherently inhibits parallelization. Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) widely dominated the RNN landscape. Although RNNs are good for capturing temporal dependencies, but they fail to preserve the complete context for long sequences. Attention mechanism overcame this drawback by providing us the ability to focus on different parts of encoded sequences during the decoding stage, thereby allowing the context to be preserved from beginning to end. Furthermore, attention between encoder and decoder has helped improve the performance of neural machine translation. Is there a way we can use attention for representations and parallelize all the computations without using RNNs?</p> <h2 id="introduction">Introduction</h2> <p>The Transformer is a deep learning language model that completely relies on attention mechanisms, specifically self-attention, to find relationships (global dependencies) between input and output. The Transformers have revolutionized the field of natural language processing and are the de facto standard for various language modeling tasks. The Transformers have been the backbone of state-of-the-art language models such as BERT, GPT, etc. Additionally, they are also being applied to computer vision and speech-related tasks. Now that we have got the idea of what the transformer is trying to achieve, so let‚Äôs dive deeper into the basic building blocks of this transformer architecture.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/transformer_overview.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/transformer_overview.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/transformer_overview.gif-1400.webp"/> <img src="/assets/img/long_live_transformers/transformer_overview.gif" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Visual overview of transformer (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">image source</a>) </div> <h2 id="model-architecture">Model Architecture</h2> <p>The Transformer also follows the encoder-decoder architecture, same as the other neural transduction models. The encoder encodes an input sequence to a sequence of continuous representations, which the decoder uses to generate output sequence each time step during decoding. Thus, the decoding stage can be thought of as autoregressive language modeling, where the decoder finds the output sequence that is most probable conditioned on the input sequence. Mathematically, it can be formulated as follows:</p> \[P_{\theta}(y | x) = \prod_{t=1}^{T} P_{\theta}(y_t | y_{\lt t}, x)\] <p>where $x$ and $y$ denote the input and output sequence, $\theta$ denote the model parameters, and $t$ denote time step in the decoding stage.</p> <div class="output"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/transformer_architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/transformer_architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/transformer_architecture-1400.webp"/> <img src="/assets/img/long_live_transformers/transformer_architecture.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Transformer Model Architecture (<a href="https://arxiv.org/abs/1706.03762">image source</a>) </div> <h3 id="encoder-decoder">Encoder-Decoder</h3> <p>The encoder consists of a stack of $N = 6$ identical layers, each containing two sub-layers, a multi-head self-attention layer, and a position-wise fully connected feed-forward network. Each sub-layer has a residual connection and layer normalization. Position-wise feed-forward network (FFN) consists of two linear transformations with ReLU activation in between. In FFN, the same linear transformation is applied across different positions. This can also be viewed as two convolutions with filter size 1.</p> \[FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\] <p>The decoder is quite similar to the encoder, except that the decoder contains two multi-head self-attention layers instead of a single layer in a stack of $N = 6$ identical layers. The first multi-head self-attention layer attends to decoder outputs generated so far and is masked in order to prevent positions from attending to future positions, whereas the second multi-head self-attention layer attends over the encoder stack output.</p> <p>The input and output sequences are embedded into a $d_{\text{model}}$ dimensional space, which is the usual step before feeding the sequence into the neural network. In addition, positional encoding is also applied to the embedded sequence, which gives a sense of order in the sequence. We‚Äôll discuss positional encoding in detail later.</p> <h3 id="self-attention">Self-Attention</h3> <p>Attention mechanisms proposed by <a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau et al., 2014</a> and <a href="https://arxiv.org/pdf/1508.04025.pdf">Luong et al., 2015</a> have been ubiquitously used to improve performance in various NLP tasks. As described previously that it is a mechanism that allows the neural network to make predictions by selectively focusing on a given sequence of data.</p> <div class="note"> <em> Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation. (Excerpt from a <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">blog post</a> by Lilian Weng). </em> </div> <p><br/></p> <p>Self-attention can be considered as a permutation invariant sequence-to-sequence operation where we map a query and a set of key-value pairs to an output. Here, the input and output in this self-attention operation are vectors. Thus, self-attention can be viewed as similar to the gating mechanism in LSTMs or GRUs, which decides how much information to store using different gates.</p> <div class="small"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/scaled_dot_prod_attn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/scaled_dot_prod_attn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/scaled_dot_prod_attn-1400.webp"/> <img src="/assets/img/long_live_transformers/scaled_dot_prod_attn.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Scaled Dot-Product Attention (<a href="https://arxiv.org/abs/1706.03762">image source</a>) </div> <p>The Transformer relies on <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> as a self-attention mechanism. Given a query matrix $Q$, a key matrix $K$, and a value matrix $V$, the output is a weighted sum of the values where the weight assigned to each value is determined by a dot product (compatibility function) of the query with the corresponding key. Mathematically, it can be expressed as follows:</p> \[\text{Attention}(Q, K, V) = \text{softmax}(\frac{Q K^T}{\sqrt{d_k}}) V \\ \text{where}\ Q \in \mathbb{R}^{\mathrm{|Q|\times d_k}}, K \in \mathbb{R}^{\mathrm{|K|\times d_k}}, V \in \mathbb{R}^{\mathrm{|V|\times d_v}}\] <p>The scaled dot-product attention faster and efficient since it is simply matrix multiplication. $\sqrt{d_k}$ is just a scaling or temperature factor which is used to normalize the dot-product in order to avoid uneven gradient flows. As we discussed above that, we perform the scaled dot-product attention on matrices due to efficient computations.</p> <p>If we were to formulate the same expression using the query $q_i$, key $k_j$ , and value $v_j$ vectors, then it would be as follows:</p> \[\text{Attention}(q_i, k_j, v_j) = \text{softmax}(\frac{q_i k_j^T}{\sqrt{d_k}}) v_j\] <p>Visually, the scaled-dot product attention can be expressed as follows:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/self_attn_calc_matrix_form-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/self_attn_calc_matrix_form-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/self_attn_calc_matrix_form-1400.webp"/> <img src="/assets/img/long_live_transformers/self_attn_calc_matrix_form.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Visual representation of self-attention calculation in matrix form (<a href="https://jalammar.github.io/illustrated-transformer/">image source</a>) </div> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>Although it may seem that the single self-attention block is sufficient to capture all contextual relevance for a particular input word in the sequence, but in practice, the word may have multiple senses, which makes capturing complete context difficult. In order to solve the above issue, the authors introduced a multi-head attention mechanism which expands the model‚Äôs ability to focus on different positions and allows to encode multiple relationships and nuances for a particular word in the sequence. In short, the multi-head attention mechanism is nothing but repeating scaled dot-product attention $h = 8$ times (i.e., over each subspace) in parallel.</p> <div class="output"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/multi_head_attn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/multi_head_attn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/multi_head_attn-1400.webp"/> <img src="/assets/img/long_live_transformers/multi_head_attn.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Multi-Head Attention (<a href="https://arxiv.org/abs/1706.03762">image source</a>) </div> <div class="note"> <em> According to the authors, multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. </em> </div> <p><br/></p> <p>In the multi-head attention layer, the inputs query matrix $Q$, key matrix $K$, and a value matrix $V$ are first linearly transformed using weight matrices $W_i^Q$, $W_i^K$, $W_i^V$ for each attention head $i$. Then, the attention head outputs computed in parallel are concatenated and linearly transformed using weight matrix $W^O$.</p> <p>Mathematically, it can be expressed as follows:</p> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O\] \[\text{where}\ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\] \[W_i^Q \in \mathbb{R}^{\mathrm{d_{model}\times d_k}}, W_i^K \in \mathbb{R}^{\mathrm{d_{model}\times d_k}}, W_i^V \in \mathbb{R}^{\mathrm{ d_{model}\times d_v}}, W_i^O \in \mathbb{R}^{\mathrm{hd_v\times d_{model}}}\] <p>Visually, the multi-head attention mechanism can be expressed as follows:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/multi_head_calc_matrix_form-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/multi_head_calc_matrix_form-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/multi_head_calc_matrix_form-1400.webp"/> <img src="/assets/img/long_live_transformers/multi_head_calc_matrix_form.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Visual representation of multi-head calculation in matrix form (<a href="https://jalammar.github.io/illustrated-transformer/">image source</a>) </div> <h3 id="positional-encoding">Positional Encoding</h3> <p>As discussed above, the self-attention is permutation invariant, and we have ditched RNNs that inherently order the sequence during processing. Therefore, we need some way to incorporate position and order of sequence. In order to achieve the above, positional encoding was introduced, which adds positional information to each word in the sequence giving a sense of order.</p> <p>A simple trivial solution would be to add index position to the word embeddings. However, there is a catch in the current solution that the word embedding values could get quite large for longer sequences and destroy the actual information in the embedding. Additionally, our model may not observe samples with specific lengths during training and result in poor generalization.</p> <p>The authors proposed a sinusoidal positional encoding which is defined as follows:</p> \[\begin{aligned} \text{PE}_{(pos, 2i)} &amp;= \text{sin}(\frac{pos}{10000^{2i/d_{model}}}) \\ \text{PE}_{(pos, 2i + 1)} &amp;= \text{cos}(\frac{pos}{10000^{2i/d_{model}}}) \end{aligned}\] <p>where $pos$ is the position and $i$ is the dimension.</p> <p>Each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions ranging from $2\pi$ to $10000 \cdot 2\pi$.</p> <p>Visually, the positional encoding can be expressed as follows:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/positional_encoding-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/positional_encoding-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/positional_encoding-1400.webp"/> <img src="/assets/img/long_live_transformers/positional_encoding.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Visual representation of positional encoding (<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">image source</a>) </div> <p>In the above diagram, we can observe that we may get the same position embedding values across different time steps for a particular dimension. For example, consider the curve dim 5 across time steps 20, 60, and 100. But if we consider curves from all the dimensions, we will end up getting different position embedding values across time steps. <em>(Please check <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">this post</a> to learn more about positional encoding in the transformer.)</em></p> <p>You must be wondering that the positional encoding information will be lost as we process the embedded sequence in higher layers. How do we ensure that the positional information is preserved? Remember, we had used residual connections in each encoder and decoder layer. These residual connections help us carry positional information to higher layers.</p> <p>Yay! we made it to the end. So let‚Äôs highlight the motivation/benefits of the self-attention mechanism based on whatever we have discussed so far.</p> <ol> <li>Computational complexity per layer is reduced due to the removal of recurrent layers</li> <li>It is trivial to parallelize the amount of computations per layer</li> <li>It provides us gating interactions observed similar to LSTMs and GRUs</li> <li>Constant path lengths between any two positions in the sequence</li> <li>It also helps in yielding more interpretable models due to multiple attention heads</li> </ol> <h2 id="follow-up-work">Follow-up Work</h2> <p>The Transformer proposed by Vaswani et al., 2017 was the first of its kind developed to process sequential data without using RNNs. Over the years, several state-of-the-art models have been derived based on the transformer architecture, and networks have become larger and larger due to the computational efficiency achieved compared to previously used RNNs. They are also now being applied to computer vision tasks described by <a href="https://arxiv.org/pdf/2012.12556.pdf">Han et al., 2021</a>.</p> <h3 id="transformer-xl">Transformer-XL</h3> <p>Although the transformer revolutionized natural language processing due to the self-attention mechanism, they have some limitations, such as fixed-length context and limited attention span. This means that model can only attend to the elements of a fixed-length segment leading to context fragmentation. Furthermore, this prevents the model from capturing long-term dependencies and allows no flow of the information across different segments.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/vanilla_transformer_training.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/vanilla_transformer_training.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/vanilla_transformer_training.gif-1400.webp"/> <img src="/assets/img/long_live_transformers/vanilla_transformer_training.gif" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Vanilla Transformer with a fixed context length at training time (<a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html">image source</a>) </div> <p>To overcome this issue, <a href="https://arxiv.org/pdf/1901.02860.pdf">Dai et al., 2019</a> proposed Transformer-XL, which allows the flow of information by reusing hidden states between segments and enabling the ability to learn long-term dependencies.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/transformer_xl_training.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/transformer_xl_training.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/transformer_xl_training.gif-1400.webp"/> <img src="/assets/img/long_live_transformers/transformer_xl_training.gif" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Transformer-XL with segment-level recurrence at training time (<a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html">image source</a>) </div> <h3 id="reformer">Reformer</h3> <p>The vanilla Transformer is very slow for processing long sequences since the self-attention takes $O(n^2)$ memory and computation where $n$ is the sequence length. Therefore, training these models is costly due to high memory and computation requirements. To overcome this limitation, <a href="https://arxiv.org/pdf/2001.04451.pdf">Kitaev et al., 2020</a> proposed Reformer, a Transformer model designed to process long sequences efficiently without much memory and computation resource.</p> <p><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">Locality Sensitive Hashing (LSH)</a> is employed to reduce the complexity of attending over long sequences. The idea behind LSH is that similar items will end up in the same buckets with high probability. We use this idea for bucketing similar vectors instead of scanning over all the pairs of vectors. The vectors in the same bucket will only attend to each other during the self-attention computation. Additionally, they also use reversible residual layers instead of standard residuals, allowing more efficient use of memory since the activations are stored only once instead of $N$ times, where $N$ is the number of layers. This reduces the complexity from $O(n^2)$ to $O(n\ \text{log}\ n)$</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/lsh_attention-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/lsh_attention-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/lsh_attention-1400.webp"/> <img src="/assets/img/long_live_transformers/lsh_attention.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Visual overview of LSH Attention (<a href="https://arxiv.org/abs/2001.04451">image source</a>) </div> <h3 id="linear-transformers">Linear Transformers</h3> <p>This work also focuses on reducing the complexity of dot-product attention and draws a parallel between Transformers and RNNs. <a href="https://arxiv.org/pdf/2006.16236.pdf">Katharopoulos et al., 2020</a> formulates the self-attention mechanism as a linear dot-product of kernel maps as follows:</p> \[\text{Attention}(Q, K, V) = \frac{\sum_{j=1}^N \text{sim}(Q_i, K_j) V_j}{\sum_{j=1}^N \text{sim}(Q_i, K_j)}\] <p>The above formulation is equivalent to the vanilla Transformer if we use the exponential dot-product as the similarity function.</p> \[\text{sim}(Q, K) = \text{exp}(\frac{Q^T K}{\sqrt{d_k}})\] <p>The authors propose to use kernel maps $\phi(x)$ to aggregate the information between all the elements in the sequence, thereby allowing us to compute the inner product between infinite-dimensional spaces efficiently. This new formulation leads to better memory and computation efficiency reducing the complexity from $O(n^2)$ to $O(n)$.</p> \[V_i^\prime = \frac{\sum_{j=1}^N \phi(Q_i)^T \phi(K_j) V_j}{\sum_{j=1}^N \phi(Q_i)^T \phi(K_j)} = \frac{\phi(Q_i)^T \sum_{j=1}^N \phi(K_j) V_j}{\phi(Q_i)^T \sum_{j=1}^N \phi(K_j)}\] <p>The above equation in the vectorized form is as follows:</p> \[(\phi(Q)\ \phi(K)^T)\ V = \phi(Q)\ (\phi(K)^T\ V)\] <p>Although the vanilla Transformers perform better than the linear Transformers, but there is a significant improvement in speed for linear Transformers. The authors have also provided a demo which you can play with at this <a href="https://linear-transformers.com/">link</a>.</p> <p>There are many follow-up works based on the vanilla Transformer architecture. Unfortunately, it is not possible to highlight every piece of work in this blog. Instead, I have tried to give intuition behind the few architectures above. I would definitely advise you to check the following <a href="https://paperswithcode.com/methods/category/transformers">webpage</a> to explore other transformer architectures as well as read the survey paper by <a href="https://arxiv.org/pdf/2106.04554.pdf">Lin et al., 2021</a>.</p> <p>If you would like to play around with the vanilla Transformer, here is the <a href="https://colab.research.google.com/github/jaygala24/pytorch-implementations/blob/master/Attention%20Is%20All%20You%20Need.ipynb">colab notebook</a> created by me for English to German translation.</p> <div class="citations"> <d-cite key="Vaswani2017AttentionIA"> <d-cite key="Kaiser2017AttentionNN"> <d-cite key="Google2017Transformer"> <d-cite key="Alammar2018Transformer"> <d-cite key="Harvard2018Transformer"> <d-cite key="weng2018attention"> <d-cite key="positional_2019_stack_exchange"> <d-cite key="positional_2019_kazemnejad"> <d-cite key="Dai2019TransformerXLAL"> <d-cite key="Google2019TransformerXL"> <d-cite key="Kitaev2020ReformerTE"> <d-cite key="Google2020Reformer"> <d-cite key="Katharopoulos2020TransformersAR"> &lt;/div&gt; </d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></div>]]></content><author><name>Jay Gala</name></author><category term="python"/><category term="pytorch"/><category term="attention"/><category term="transformers"/><summary type="html"><![CDATA[This blog post explains the transformer and its building blocks as well as how they changed the field of NLP.]]></summary></entry><entry><title type="html">Word Embeddings</title><link href="https://jaygala24.github.io/blog/2021/word_embeddings/" rel="alternate" type="text/html" title="Word Embeddings"/><published>2021-04-20T00:00:00+00:00</published><updated>2021-04-20T00:00:00+00:00</updated><id>https://jaygala24.github.io/blog/2021/word_embeddings</id><content type="html" xml:base="https://jaygala24.github.io/blog/2021/word_embeddings/"><![CDATA[<p>Humans use language as a way of communication for exchanging ideas and knowledge with others. Words are an integral part of the language that represents the <a href="https://en.wikipedia.org/wiki/Denotational_semantics">denotational semantics</a>, i.e., the meaning of a word. Humans are good at processing and understanding the idea that the words convey. Hence, we can share information about an image or an incident using a short string, assuming that we have some previous context. For example, a single word, ‚Äútraffic,‚Äù conveys the information equivalent to a picture representing several vehicles stuck in a jam as shown in the figure below. However, computers are not good at understanding the ideas from the words, and we need to way to encode these ideas which computers can understand, i.e., in the form of numbers. These encoded representation are helpful for solving complex NLP tasks and are known as word embeddings (word vectors of a certain dimensions representing the idea of the word). This encoding is known as word embeddings (word vectors) which can help us use these representations for solving the complex NLP tasks.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/traffic_jam_pic-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/traffic_jam_pic-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/traffic_jam_pic-1400.webp"/> <img src="/assets/img/word_embeddings/traffic_jam_pic.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Picture of a traffic jam (<a href="https://bit.ly/3aq0qL9">image source</a>) </div> <h2 id="word2vec">Word2Vec</h2> <h3 id="overview">Overview</h3> <p>Previously, neural language models involved the first stage as learning a distributed representation for words and using these representations in the later stages for obtaining prediction. The main idea of the word2vec is also based on these neural language models where we use the hidden layer of a neural network to learn continuous representations of words which we call embeddings. Here, we discard the output layer of a trained neural network. Word2Vec model presents two algorithms:</p> <ul> <li>Continuous Bag of Words (CBOW): Predict center word based on the context words.</li> <li>Skip Gram: Predict the context words based on the center word.</li> </ul> <p>The figure below illustrates the two algorithms:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/word2vec_algorithms-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/word2vec_algorithms-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/word2vec_algorithms-1400.webp"/> <img src="/assets/img/word_embeddings/word2vec_algorithms.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Word2Vec Model Architectures (<a href="https://arxiv.org/abs/1301.3781">image source</a>) </div> <h3 id="skip-gram-model">Skip-Gram Model</h3> <p>From now onwards, we will look into the skip-gram model for word embeddings. The task is formulated as predicting the context words within a fixed window of size m given the center word. The visual illustration of above idea is shown below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/skip_gram_overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/skip_gram_overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/skip_gram_overview-1400.webp"/> <img src="/assets/img/word_embeddings/skip_gram_overview.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Skip-Gram Model Overview (<a href="https://stanford.io/32wNQWe">image source</a>) </div> <p>Word pairs from the large corpus of text for a fixed window size $m$ are used for training the neural network. These word pairs are formed by looking at a fixed window size $m$ before and after the center word. This window size is a hyperparameter that you can play around with, but the authors found that window size 5 seems to work well in general. Having a smaller window size means that you are capturing minimal context. On the other hand, if your window size is too large, you are capturing too much context, which might not help you obtain specific meanings. For the above example with window size 2, the training pairs would be the following.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    [(into, problems), (into, turning), (into, banking), (into, crises)]
</code></pre></div></div> <p>Since every word is represented by a vector, so the objective is to iteratively maximize the probability of context words $o$ given the center word $c$ for each position $t$ and adjust the word vectors.</p> \[L(\theta) = \prod_{t=1}^{T}\prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j}\ |\ w_t\ ;\ \theta)\] <p>where $L$ is the likelihood estimation and $\theta$ is the vector representation to be optimized.</p> <p>In order to avoid the floating-point overflow and simple gradient calculation, we take the apply logarithm to the above likelihood estimation. The cost function is given as follows:</p> \[J(\theta) = - \frac{1}{T}\ log\ L(\theta) = - \frac{1}{T}\ \prod_{t=1}^{T}\prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j}\ |\ w_t\ ;\ \theta)\] <p>Here we are minimizing the cost function, which means that we are maximizing the likelihood estimation, i.e., predictive accuracy.</p> <p>Likelihood estimation for a context word $o$ given the center word $c$ is as follows:</p> \[P(o\ |\ c) = \frac{exp(u_o^T v_c)}{\sum_{w \in V}\ exp(u_w^T v_c)}\] <p>where</p> <ul> <li>$v_w$ and $u_w$ are the center word and context word vector representations</li> <li>$u_o^T v_c$ represents the dot product which is used as a similarity measure between context word $o$ and center word $c$</li> <li>$V$ represents the vocabulary</li> </ul> <p>In order to express this similarity measure in terms of probability, we normalize over the entire vocabulary (the idea of using softmax) and $exp$ is used to quantify the dot product to a positive value.</p> <p>Computing the normalizing factor for every word is too much expensive, which is why the authors came up with some tricks which reduce the computational cost and speed up the training.</p> <h3 id="negative-sampling">Negative Sampling</h3> <p>The main idea of the negative sampling is to differentiate data from noise, i.e., train a binary logistic regression for classifying a true pair (center word and context word) against several noise pairs (center word and random word). So now our problem is reduced to $K + 1$ labels classification instead of $V$ words ($K \ll V$), which means that weights will only be updated for $K + 1$ words whereas weights for all the words were updated. In general, we choose 5 negative words other than the context window around the center word ($K = 5$). We want the context words to have a higher probability than the sampled negative words.</p> <p>The new objective function (cost function) is given as follows:</p> \[J_{neg-sample}(\theta) = -\ log\ (\sigma(u_o^T v_c)) - \sum_{k=1}^{K}\ log\ (\sigma(-u_k^T v_c))\] <p>where</p> <ul> <li>$\sigma$ represents sigmoid</li> <li>first term represents the estimation for true pair</li> <li>second term represents the estimation for negative samples</li> </ul> <p>The authors found that the unigram distribution $U(w)^{3/4}$ works well than the other unigram and uniform distribution choices for sampling noise. The intuition is that raised to $3/4$ factor brings down the probability for more frequent words.</p> \[P_n(w) = \frac{U(w)^{3/4}}{Z}\] <p>where $Z$ is the normalization term.</p> <h3 id="subsampling-frequent-words">Subsampling Frequent Words</h3> <p>Word2vec has been trained on a very large corpus of text in which frequently occurring words do not contribute significantly to the meaning of a word. Common function words such as ‚Äúthe‚Äù, ‚Äúas‚Äù, ‚Äúa‚Äù provide structure to the sentence but don‚Äôt help in learning good quality word representation as they occur in context with many words in the corpus. For example, the co-occurrence of ‚ÄúNew‚Äù, ‚ÄúYork‚Äù benefits the model in capturing better meaningful representation than the co-occurrence of ‚ÄúNew‚Äù, ‚Äúthe‚Äù. The authors introduce a subsampling technique that discards the high-frequency words based on the probability formula computed for each word $w_i$ which is given below:</p> \[P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}\] <p>where $t$ is a chosen threshold, typically around $10^{-5}$.</p> <h3 id="skip-gram-implementation">Skip-Gram Implementation</h3> <p>Here we will be using text corpus of cleaned wikipedia articles provided by Matt Mahoney.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">s3</span><span class="p">.</span><span class="n">amazonaws</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">video</span><span class="p">.</span><span class="n">udacity</span><span class="o">-</span><span class="n">data</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">topher</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="n">October</span><span class="o">/</span><span class="mi">5</span><span class="n">bbe6499_text8</span><span class="o">/</span><span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">"</span><span class="s">retina</span><span class="sh">"</span>

<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check if gpu is available since training is faster
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Word2VecDataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Prepares the training data for the word2vec neural network.
            Params:
                corpus (string): corpus of words
                min_count (int): words with minimum occurrence to consider
                window_size (int): context window size for generating word pairs
                threshold (float): threshold used for subsampling words
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
        <span class="n">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="c1"># only consider the words that occur atleast 5 times in the corpus 
</span>        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">({</span><span class="n">word</span><span class="p">:</span><span class="n">count</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">})</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="nf">most_common</span><span class="p">())}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

        <span class="c1"># create prob dist based on word frequency
</span>        <span class="n">word_freq</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">unigram_dist</span> <span class="o">=</span> <span class="n">word_freq</span> <span class="o">/</span> <span class="n">word_freq</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

        <span class="c1"># create prob dist for negative sampling
</span>        <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">unigram_dist</span> <span class="o">**</span> <span class="mf">0.75</span>
        <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

        <span class="c1"># get prob for drop words
</span>        <span class="n">self</span><span class="p">.</span><span class="n">word_drop_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">threshold</span> <span class="o">/</span> <span class="n">word_freq</span><span class="p">)</span>

        <span class="c1"># create the training corpus subsampling frequent words
</span>        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> 
                          <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span> <span class="ow">and</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">word_drop_prob</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]]]</span>

        <span class="c1"># create word pairs for corpus
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">generate_word_pairs</span><span class="p">()</span>
    

    <span class="k">def</span> <span class="nf">generate_word_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the pairs of center and context words based on the context window size.
        </span><span class="sh">"""</span>
        <span class="n">word_pair_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">current_idx</span><span class="p">,</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="c1"># find the start and end of context window
</span>            <span class="n">left_boundary</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">right_boundary</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">))</span>

            <span class="c1"># obtain the context words and center words based on context window
</span>            <span class="n">context_word_ids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">left_boundary</span><span class="p">:</span><span class="n">current_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">right_boundary</span><span class="p">]</span>
            <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span><span class="p">]</span>
            
            <span class="c1"># add the word pair to the training set
</span>            <span class="k">for</span> <span class="n">context_word_id</span> <span class="ow">in</span> <span class="n">context_word_ids</span><span class="p">:</span>
                <span class="n">word_pair_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">center_word_id</span><span class="p">,</span> <span class="n">context_word_id</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">word_pair_ids</span> <span class="o">=</span> <span class="n">word_pair_ids</span>


    <span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the batches for training the network.
            Params:
                batch_size (int): size of the batch
            Returns:
                batch (torch tensor of shape (batch_size, 2)): tensor of word pair ids for a given batch
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">word_pair_ids</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">word_pair_ids</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">get_negative_samples</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Samples negative word ids for a given batch.
            Params:
                batch_size (int): size of the batch
                n_samples (int): number of negative samples
            Returns:
                neg_samples (torch tensor of shape (batch_size, n_samples)): tensor of negative sample word ids
                    for a given batch
        </span><span class="sh">"""</span>
        <span class="n">neg_samples_ids</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">),</span> 
                                       <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">neg_samples_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read the file and initialize the Word2VecDataset
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">text8</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nc">Word2VecDataset</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SkipGramModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Skip Gram variant of Word2Vec with negative sampling for learning word 
            embeddings. Uses the concept of predicting context words given the 
            center word.
            Params:
                vocab_size (int): number of words in the vocabulary
                embed_dim (int): embeddings of dimension to be generated
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SkipGramModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="c1"># embedding layers for input (center) and output (context) words
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1"># initialize the embeddings with uniform dist
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_ids</span><span class="p">,</span> <span class="n">pos_out_ids</span><span class="p">,</span> <span class="n">neg_out_ids</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Trains the Skip Gram variant model and updates the weights based on the
            criterion.
            Params:
                in_ids (torch tensor of shape (batch_size,)): indexes of the input words for a batch
                pos_out_ids (torch tensor of shape (batch_size,)): indexes of the output words (true pairs) for a batch
                neg_out_ids (torch tensor of shape (batch_size, number of negative samples)): 
                    indexes of the noise words (negative pairs) for a batch
        </span><span class="sh">"""</span>
        <span class="n">emb_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_in</span><span class="p">(</span><span class="n">in_ids</span><span class="p">)</span>
        <span class="n">pos_emb_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_out</span><span class="p">(</span><span class="n">pos_out_ids</span><span class="p">)</span>
        <span class="n">neg_emb_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_out</span><span class="p">(</span><span class="n">neg_out_ids</span><span class="p">)</span>

        <span class="c1"># calculate loss for true pair
</span>        <span class="c1"># ----------------------------
</span>        <span class="c1"># step 1 is calculate the dot product between the input and output word embeddings
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">pos_emb_out</span><span class="p">,</span> <span class="n">emb_in</span><span class="p">)</span>      <span class="c1"># element-wise multiplication
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">pos_loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>           <span class="c1"># sum the element-wise components
</span>        
        <span class="c1"># step 2 is to calculate the log sogmoid of dot product
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">logsigmoid</span><span class="p">(</span><span class="n">pos_loss</span><span class="p">)</span>

        <span class="c1"># calculate loss for negative pairs
</span>        <span class="c1"># ----------------------------------
</span>        <span class="c1"># step 1 is calculate the dot product between the input and output word embeddings
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="o">-</span><span class="n">neg_emb_out</span><span class="p">,</span> <span class="n">emb_in</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">()</span>   <span class="c1"># matrix-matrix multiplication
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">neg_loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                               <span class="c1"># sum the element-wise components
</span>
        <span class="c1"># step 2 is to calculate the log sogmoid of dot product
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">logsigmoid</span><span class="p">(</span><span class="n">neg_loss</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">pos_loss</span> <span class="o">+</span> <span class="n">neg_loss</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># intialize the model and optimizer
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">SkipGramModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training the network
</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_neg_samples</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Start of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">get_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># get the negative samples
</span>        <span class="n">noise_word_ids</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">get_negative_samples</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">n_neg_samples</span><span class="p">)</span>

        <span class="c1"># load tensor to GPU
</span>        <span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target_word_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">noise_word_ids</span> <span class="o">=</span> <span class="n">noise_word_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># forward pass
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">,</span> <span class="n">target_word_ids</span><span class="p">,</span> <span class="n">noise_word_ids</span><span class="p">)</span>

        <span class="c1"># backward pass, optimize
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epochs: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="se">\t</span><span class="s">Avg training loss: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="se">\t</span><span class="s">Ellapsed time: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> s</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">End of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the trained embeddings from the model
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># number of words to be visualized
</span><span class="n">viz_words</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># projecting the embedding dimension from 300 to 2
</span><span class="n">tsne</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">()</span>
<span class="n">embed_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[:</span><span class="n">viz_words</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># plot the projected embeddings
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">viz_words</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">idx2word</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="p">(</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div></div> <h2 id="glove">GloVe</h2> <h3 id="overview-1">Overview</h3> <p>Previously, there were two main directions for learning distributed word representations: 1) count-based methods such as Latent Semantic Analysis (LSA) 2) direct prediction-based methods such as Word2Vec. Count-based methods make efficient use of statistical information about the corpus, but they do not capture the meaning of the words like word2vec and perform poorly on analogy tasks such as <code class="language-plaintext highlighter-rouge">king - queen = man - woman</code>. On the other hand, direct prediction-based methods capture the meaning of the word semantically and syntactically using local context but fail to consider the global count statistics. This is where GloVe comes into the picture and overcomes the drawbacks of both approaches by combining them. The author proposed a global log bilinear regression model to learn embeddings based on the co-occurrence of words. Note that the GloVe does not use a neural network for learning word vectors.</p> <h3 id="co-occurrence-matrix">Co-occurrence matrix</h3> <p>The authors used a co-occurrence matrix with a context window of fixed size $m$ to learn the word embeddings. Let‚Äôs try to generate this matrix for the below toy example with a context window of size 2:</p> <ul> <li>I like deep learning</li> <li>I like NLP</li> <li>I enjoy flying</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/co_occurrence_matrix-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/co_occurrence_matrix-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/co_occurrence_matrix-1400.webp"/> <img src="/assets/img/word_embeddings/co_occurrence_matrix.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Co-occurrence Matrix Example (<a href="https://stanford.io/3n4FH4H">image source</a>) </div> <h3 id="mathematics">Mathematics</h3> <p>Before we move ahead, let‚Äôs get familiarized with some notations.</p> <ul> <li>$X$ denotes the word-word co-occurrence matrix</li> <li>$X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$</li> <li>$X_i$ = $\sum_{k}{X_{ik}}$ denotes the number of times any word $k$ appearing in context of word $i$ and $k$ represents the total number of distinct words that appear in context of word $i$)</li> <li>$P_{ij} = P(j | i) = \frac{X_{ij}}{X_i}$ denotes the co-occurence probablity i.e. probability that word $j$ appears in the context of word $i$</li> </ul> <p>The denominator term in the co-occurrence probability accounts for global statistics, which word2vec does not uses. The main idea behind the GloVe is to encode meaning using the ratios of co-occurrence probabilities. Let‚Äôs understand the above by deriving the linear meaning components for the following words based on co-occurrence probability.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/co_occurrence_probs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/co_occurrence_probs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/co_occurrence_probs-1400.webp"/> <img src="/assets/img/word_embeddings/co_occurrence_probs.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Co-occurrence Probabilities Example (<a href="http://nlp.stanford.edu/pubs/glove.pdf">image source</a>) </div> <p>The matrix shows the co-occurrence probabilities for the words from the concept of the thermodynamic phases of water (i.e., ‚Äúice‚Äù and ‚Äústeam‚Äù). The first two rows represent the co-occurrence probabilities for the words $ice$ and ‚Äústeam‚Äù, whereas the last row represents their ratios. We can observe the following:</p> <ul> <li>ratio is not neural for closely related words such as ‚Äúsolid‚Äù and ‚Äúice‚Äù or ‚Äúgas‚Äù and ‚Äústeam‚Äù</li> <li>ratio is neutral for words relevant to ‚Äúice‚Äù and ‚Äústeam‚Äù both or not completely irrelevant to both</li> </ul> <p>The ratio of co-occurrence proababilities is a good starting point for learning word embeddings. Let‚Äôs start with the most general function $F$ parametrized by 3 word vectors ($w_i$, $w_j$ and $\tilde{w_k}$) given below.</p> \[F(w_i, w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}\] <p>where $w, \tilde{w} \in \mathrm{R^d}$ and $\tilde{w}$ represent the separate context words.</p> <p>How do we choose $F$?</p> <p>There can be many possibilities for choosing $F$ but imposing some constraints allows us to restrict $F$ and select a unique choice. The goal is to learn word vectors (embeddings) that can be projected in the word vector space. These vector spaces are inherently linear, i.e., think of vectors as a line in $\mathrm{R^d}$ space, so the most intuitive way is to take vector differences which makes our function $F$ as follows:</p> \[F(w_i - w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}\] <p>We see that the right-hand side of the above equation is a scalar. Choosing a complex function such as a neural network would introduce non-linearities since our primary goal is to capture the linear meaning components from word vector space. Here, we take dot product on the left-hand side to make it a scalar similar to the right-hand side.</p> \[F((w_i - w_j)^T \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}\] <p>We also need to preserve symmetry for the distinction between a word and a context word which means that if $ice$ can be used as a context word for $water$, then $water$ can also be used as a context word for $ice$. In a simple, it can be expressed as $w \leftrightarrow \tilde{w}$. This is also evident from our co-occurrence matrix since $X \leftrightarrow X^T$. In order to restore the symmetry, we require that function $F$ is a homomorphism between groups $(\mathrm{R, +})$ and $(\mathrm{R, \times})$.</p> <div class="note"> <em> Given two groups, $\small (G, ‚àó)$ and $\small (H, \cdot)$, a group homomorphism from $\small (G, ‚àó)$ to $\small (H, \cdot)$ is a function $\small h : G \rightarrow H$ such that for all $u$ and $v$ in $\small G$ it holds that $\small h(u * v) = h(u) \cdot h(v)$. </em> </div> \[\begin{align*} F((w_i - w_j)^T \tilde{w_k}) &amp;= F(w_i^T \tilde{w_k} + (-w_j^T \tilde{w_k})) \\ &amp;= F(w_i^T \tilde{w_k}) \times F(-w_j^T \tilde{w_k}) \\ &amp;= F(w_i^T \tilde{w_k}) \times F(w_j^T \tilde{w_k})^{-1} \\ &amp;= \frac{F(w_i^T \tilde{w_k})}{F(w_j^T \tilde{w_k})} \end{align*}\] <p>So if we recall the $F$ in terms of co-occurrence probabilities, we get the following.</p> \[F(w_i^T \tilde{w_k}) = P_{ik} = \frac{X_{ik}}{X_i}\] <p>Since we are expressing $F$ in terms of probability which is a non-negative term, so we apply exponential to dot product $w_i^T \tilde{w_k}$ and then take logarithm on both sides.</p> \[w_i^T \tilde{w_k} = log(P_{ik}) = log(X_{ik}) - log(X_i)\] <p>On the right hand, the term $log(X_i)$ is independent of $k$ so it can be absorbed into a bias $b_i$ for $w_i$. Finally, we add bias $\tilde{b_k}$ for $\tilde{w_k}$ to restore the symmetry.</p> \[w_i^T \tilde{w_k} + b_i + \tilde{b_k} = log(X_{ik})\] <p>The above equation leads to our objective function, a weighted least squares regression model where we use the weighting function $f(X_{ij})$ for word-word co-occurrences.</p> \[J = \sum_{i,j = 1}^{V}f(X_{ij}) (w_i^T \tilde{w_k} + b_i + \tilde{b_k} - logX_{ik})^2\] <p>where $V$ is the size of the vocabulary.</p> <p>Here, the weighting function is defined as follows:</p> \[f(x) = \begin{cases} (x / x_{max})^{\alpha} &amp; \text{if}\ x &lt; x_{max} \\ 1 &amp; \text{otherwise} \end{cases}\] <p>where $x_{max}$ is the cutoff of the weighting function and $\alpha$ is power scaling similar to Word2Vec.</p> <h3 id="glove-implementation">GloVe Implementation</h3> <p>Here we will be using text corpus of cleaned wikipedia articles provided by Matt Mahoney.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">s3</span><span class="p">.</span><span class="n">amazonaws</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">video</span><span class="p">.</span><span class="n">udacity</span><span class="o">-</span><span class="n">data</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">topher</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="n">October</span><span class="o">/</span><span class="mi">5</span><span class="n">bbe6499_text8</span><span class="o">/</span><span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">"</span><span class="s">retina</span><span class="sh">"</span>

<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check if gpu is available since training is faster
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GloVeDataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Prepares the training data for the glove model.
            Params:
                corpus (string): corpus of words
                min_count (int): words with minimum occurrence to consider
                window_size (int): context window size for generating co-occurrence matrix
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="c1"># only consider the words that occur more than 5 times in the corpus 
</span>        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">({</span><span class="n">word</span><span class="p">:</span><span class="n">count</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">})</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="nf">most_common</span><span class="p">())}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

        <span class="c1"># create the training corpus
</span>        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">]</span>

        <span class="c1"># create the co-occurrence matrix for corpus
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">create_cooccurrence_matrix</span><span class="p">()</span>


    <span class="k">def</span> <span class="nf">create_cooccurrence_matrix</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the co-occurence matrix of center and context words based on the context window size.
        </span><span class="sh">"""</span>
        <span class="n">cooccurrence_counts</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="n">Counter</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">current_idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="c1"># find the start and end of context window
</span>            <span class="n">left_boundary</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">right_boundary</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">))</span>

            <span class="c1"># obtain the context words and center words based on context window
</span>            <span class="n">context_word_ids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">left_boundary</span><span class="p">:</span><span class="n">current_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">right_boundary</span><span class="p">]</span>
            <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">context_word_id</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">context_word_ids</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">current_idx</span> <span class="o">!=</span> <span class="n">idx</span><span class="p">:</span>
                    <span class="c1"># add (1 / distance from center word) for this pair
</span>                    <span class="n">cooccurrence_counts</span><span class="p">[</span><span class="n">center_word_id</span><span class="p">][</span><span class="n">context_word_id</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nf">abs</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">-</span> <span class="n">idx</span><span class="p">)</span>
        
        <span class="c1"># create tensors for input word ids, output word ids and their co-occurence count
</span>        <span class="n">in_ids</span><span class="p">,</span> <span class="n">out_ids</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">center_word_id</span><span class="p">,</span> <span class="n">counter</span> <span class="ow">in</span> <span class="n">cooccurrence_counts</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">context_word_id</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counter</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">in_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">center_word_id</span><span class="p">)</span>
                <span class="n">out_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">context_word_id</span><span class="p">)</span>
                <span class="n">counts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">in_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">in_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">out_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cooccurrence_counts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the batches for training the network.
            Params:
                batch_size (int): size of the batch
            Returns:
                batch (torch tensor of shape (batch_size, 3)): tensor of word pair ids and 
                    co-occurence counts for a given batch
        </span><span class="sh">"""</span>
        <span class="n">random_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_ids</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_ids</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">random_ids</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">random_ids</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="k">yield</span> <span class="n">self</span><span class="p">.</span><span class="n">in_ids</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">out_ids</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">cooccurrence_counts</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read the file and initialize the GloVeDataset
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">text8</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nc">GloVeDataset</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GloVeModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">x_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> GloVe model for learning word embeddings. Uses the approach of predicting 
            context words given the center word.
            Params:
                vocab_size (int): number of words in the vocabulary
                embed_dim (int): embeddings of dimension to be generated
                x_max (int): cutoff of the weighting function
                alpha (int): parameter of the weighting funtion
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">GloVeModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_max</span> <span class="o">=</span> <span class="n">x_max</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

        <span class="c1"># embedding layers for input (center) and output (context) words along with biases
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_in</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># initialize the embeddings with uniform dist and set bias to zero
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>

    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_ids</span><span class="p">,</span> <span class="n">out_ids</span><span class="p">,</span> <span class="n">cooccurrence_counts</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Trains the GloVe model and updates the weights based on the
            criterion.
            Params:
                in_ids (torch tensor of shape (batch_size,)): indexes of the input words for a batch
                out_ids (torch tensor of shape (batch_size,)): indexes of the output words for a batch
                cooccurrence_counts (torch tensor of shape (batch_size,)): co-occurence count of input 
                    and output words for a batch
        </span><span class="sh">"""</span>
        <span class="n">emb_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_in</span><span class="p">(</span><span class="n">in_ids</span><span class="p">)</span>
        <span class="n">emb_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_out</span><span class="p">(</span><span class="n">out_ids</span><span class="p">)</span>
        <span class="n">b_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bias_in</span><span class="p">(</span><span class="n">in_ids</span><span class="p">)</span>
        <span class="n">b_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bias_out</span><span class="p">(</span><span class="n">out_ids</span><span class="p">)</span>

        <span class="c1"># add 1 to counts i.e. cooccurrences in order to avoid log(0) case
</span>        <span class="n">cooccurrence_counts</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># count weight factor
</span>        <span class="n">weight_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">cooccurrence_counts</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">x_max</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">weight_factor</span><span class="p">[</span><span class="n">cooccurrence_counts</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="c1"># calculate the distance between the input and output embeddings
</span>        <span class="n">emb_prods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">emb_in</span> <span class="o">*</span> <span class="n">emb_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_cooccurrences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">cooccurrence_counts</span><span class="p">)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="p">(</span><span class="n">emb_prods</span> <span class="o">+</span> <span class="n">b_in</span> <span class="o">+</span> <span class="n">b_out</span> <span class="o">-</span> <span class="n">log_cooccurrences</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">weight_factor</span> <span class="o">*</span> <span class="n">distances</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># intialize the model and optimizer
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">GloVeModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adagrad</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training the network
</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Start of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">input_word_ids</span><span class="p">,</span> <span class="n">target_word_ids</span><span class="p">,</span> <span class="n">cooccurrence_counts</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">get_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># load tensor to GPU
</span>        <span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">input_word_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target_word_ids</span> <span class="o">=</span> <span class="n">target_word_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">cooccurrence_counts</span> <span class="o">=</span> <span class="n">cooccurrence_counts</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># forward pass
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">,</span> <span class="n">target_word_ids</span><span class="p">,</span> <span class="n">cooccurrence_counts</span><span class="p">)</span>

        <span class="c1"># backward pass, optimize
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epochs: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="se">\t</span><span class="s">Avg training loss: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="se">\t</span><span class="s">Ellapsed time: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> s</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">End of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the trained embeddings from the model
</span><span class="n">emb_in</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">emb_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">embed_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">emb_in</span> <span class="o">+</span> <span class="n">emb_out</span>

<span class="c1"># number of words to be visualized
</span><span class="n">viz_words</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># projecting the embedding dimension from 300 to 2
</span><span class="n">tsne</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">()</span>
<span class="n">embed_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[:</span><span class="n">viz_words</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># plot the projected embeddings
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">viz_words</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">idx2word</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="p">(</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div></div> <div class="citations"> <d-cite key="Mikolov2013EfficientEO"> <d-cite key="Mikolov2013DistributedRO"> <d-cite key="mccormick2016Word2Vec"> <d-cite key="mccormick2017Word2Vec"> <d-cite key="cs224n_word2vec"> <d-cite key="kaggle_word2vec"> <d-cite key="Pennington2014GloVeGV"> <d-cite key="group_homomorphism_wiki"> <d-cite key="glove_stack_exchange"> <d-cite key="gauthier_glove"> <d-cite key="gavrilov_glove"> &lt;/div&gt; </d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></div>]]></content><author><name>Jay Gala</name></author><category term="python"/><category term="pytorch"/><category term="word-embeddings"/><category term="word2vec"/><category term="glove"/><summary type="html"><![CDATA[This blog goes through the two popular techniques used for embeddings i.e. Word2Vec and GloVe in detail understanding the math along with the code implementation in PyTorch.]]></summary></entry><entry><title type="html">Getting Started with Kaggle Competitions: Melanoma Classification Challenge</title><link href="https://jaygala24.github.io/blog/2020/kaggle-melanoma-challenge/" rel="alternate" type="text/html" title="Getting Started with Kaggle Competitions: Melanoma Classification Challenge"/><published>2020-12-02T00:00:00+00:00</published><updated>2020-12-02T00:00:00+00:00</updated><id>https://jaygala24.github.io/blog/2020/kaggle-melanoma-challenge</id><content type="html" xml:base="https://jaygala24.github.io/blog/2020/kaggle-melanoma-challenge/"><![CDATA[<p>üìå Note: The authors of this blog post jointly participated in the Kaggle competition as a team.</p> <p>This post assumes that you are acquainted with the basic skills of working with <a href="https://pytorch.org/">PyTorch</a>. If you are new to PyTorch, we would highly encourage you to go through <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep Leaning With PyTorch: A 60 Minute Blitz by PyTorch</a>. It‚Äôs a great place for beginners to get your hands dirty.</p> <h2 id="download-data">Download Data</h2> <p>Here we will be using the preprocessed images by <a href="https://www.kaggle.com/arroqc/siic-isic-224x224-images">Arnaud Roussel</a> due to storage limitations on Google Colab.</p> <p>Now let‚Äôs download the preprocessed image dataset using the Kaggle API. Remember to add your <code class="language-plaintext highlighter-rouge">USERNAME</code> and <code class="language-plaintext highlighter-rouge">API_KEY</code> in the code block below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">kaggle</span> <span class="o">-</span><span class="n">q</span>
<span class="err">!</span><span class="n">mkdir</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="p">.</span><span class="n">kaggle</span>
<span class="err">!</span><span class="n">echo</span> <span class="sh">'</span><span class="s">{</span><span class="sh">"</span><span class="s">username</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">YOUR_USERNAME</span><span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="s">key</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">YOUR_API_KEY</span><span class="sh">"</span><span class="s">}</span><span class="sh">'</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="p">.</span><span class="n">kaggle</span><span class="o">/</span><span class="n">kaggle</span><span class="p">.</span><span class="n">json</span>
<span class="err">!</span><span class="n">chmod</span> <span class="mi">600</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="p">.</span><span class="n">kaggle</span><span class="o">/</span><span class="n">kaggle</span><span class="p">.</span><span class="n">json</span>
<span class="err">!</span><span class="n">kaggle</span> <span class="n">datasets</span> <span class="n">download</span> <span class="o">-</span><span class="n">d</span> <span class="n">arroqc</span><span class="o">/</span><span class="n">siic</span><span class="o">-</span><span class="n">isic</span><span class="o">-</span><span class="mi">224</span><span class="n">x224</span><span class="o">-</span><span class="n">images</span>
<span class="err">!</span><span class="n">mkdir</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">siic</span><span class="o">-</span><span class="n">isic</span><span class="o">-</span><span class="mi">224</span><span class="n">x224</span><span class="o">-</span><span class="n">images</span>
<span class="err">!</span><span class="n">unzip</span> <span class="o">-</span><span class="n">q</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">siic</span><span class="o">-</span><span class="n">isic</span><span class="o">-</span><span class="mi">224</span><span class="n">x224</span><span class="o">-</span><span class="n">images</span><span class="p">.</span><span class="nb">zip</span> <span class="o">-</span><span class="n">d</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">siic</span><span class="o">-</span><span class="n">isic</span><span class="o">-</span><span class="mi">224</span><span class="n">x224</span><span class="o">-</span><span class="n">images</span>
</code></pre></div></div> <p>Download the csv files from the <a href="https://www.kaggle.com/c/siim-isic-melanoma-classification/data">competition page</a> and place this files in the <code class="language-plaintext highlighter-rouge">content</code> directory.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">pip</span> <span class="o">-</span><span class="n">q</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">efficientnet_pytorch</span> <span class="n">pretrainedmodels</span> <span class="o">-</span><span class="n">q</span>
</code></pre></div></div> <h2 id="what-is-melanoma">What is Melanoma?</h2> <p>Malignant Melanoma is a type of skin cancer that develops from pigment-producing cells known as melanocytes.</p> <p>The skin cells found in the upper layer of the skin are termed as Melanocytes. These produce a pigment Melanin, which is the pigment that is responsible for skin color. Exposure to UV radiation from the sun or tanning beds causes skin damage as it triggers these melanocytes to increase the secretion of Melanin.</p> <p>Melanoma occurs when there is DNA damage caused by burning or tanning due to UV exposure, triggering mutations in the melanocytes leading to unrestricted cellular growth.</p> <h2 id="objective">Objective</h2> <p>The objective of this competition is to identify melanoma in images of skin lesions. In particular, we need to use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.</p> <p>Melanoma is a deadly disease, but if detected at an early stage, most melanomas can be cured with minor surgery.</p> <p>This competition is aimed at building a Classification Model that can predict whether the onset of malignant Melanoma from lesion images.</p> <p>In short, we need to create a classification model that is capable of distinguishing whether the lesion in the image is benign (class 0) or malignant (class 1).</p> <p>This will be very helpful to detect the early signs so that further medical attention can be made available to the patient.</p> <p>Now let‚Äôs import the necessary packages below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="n">albumentations</span>
<span class="kn">import</span> <span class="n">pretrainedmodels</span>
<span class="kn">from</span> <span class="n">efficientnet_pytorch</span> <span class="kn">import</span> <span class="n">EfficientNet</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</code></pre></div></div> <p>Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using <code class="language-plaintext highlighter-rouge">torch.cuda.is_available()</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># device configuration
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h2 id="about-the-dataset">About the Dataset</h2> <p>The dataset consists of images and metadata, which are described as follows:</p> <ul> <li>Images: DICOM, JPEG, TFRecord formats</li> <li>Metadata: image_name, patient_id, sex, age_approx, anatom_site_general_challenge, diagnosis, benign_malignant, target</li> </ul> <p>Let‚Äôs take a look at the dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_images_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./siic-isic-224x224-images/train/</span><span class="sh">'</span>
<span class="n">test_images_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./siic-isic-224x224-images/test/</span><span class="sh">'</span>
<span class="n">train_df_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./train.csv</span><span class="sh">'</span>
<span class="n">test_df_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./test.csv</span><span class="sh">'</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">train_df_path</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">test_df_path</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th>¬†</th> <th>image_name</th> <th>patient_id</th> <th>sex</th> <th>age_approx</th> <th>anatom_site_general_challenge</th> <th>diagnosis</th> <th>benign_malignant</th> <th>target</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>ISIC_2637011</td> <td>IP_7279968</td> <td>male</td> <td>45.0</td> <td>head/neck</td> <td>unknown</td> <td>benign</td> <td>0</td> </tr> <tr> <td>1</td> <td>ISIC_0015719</td> <td>IP_3075186</td> <td>female</td> <td>45.0</td> <td>upper extremity</td> <td>unknown</td> <td>benign</td> <td>0</td> </tr> <tr> <td>2</td> <td>ISIC_0052212</td> <td>IP_2842074</td> <td>female</td> <td>50.0</td> <td>lower extremity</td> <td>nevus</td> <td>benign</td> <td>0</td> </tr> <tr> <td>3</td> <td>ISIC_0068279</td> <td>IP_6890425</td> <td>female</td> <td>45.0</td> <td>head/neck</td> <td>unknown</td> <td>benign</td> <td>0</td> </tr> <tr> <td>4</td> <td>ISIC_0074268</td> <td>IP_8723313</td> <td>female</td> <td>55.0</td> <td>upper extremity</td> <td>unknown</td> <td>benign</td> <td>0</td> </tr> </tbody> </table> <p>Let‚Äôs take a look at the number of samples in the train and test set.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Train data shape: </span><span class="si">{</span><span class="n">train_df</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test data shape: </span><span class="si">{</span><span class="n">test_df</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train data shape: (33126, 8)
Test data shape: (10982, 6)
</code></pre></div></div> <p>Let‚Äôs take a look at the missing value count for each attribute.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_name                         0
patient_id                         0
sex                               65
age_approx                        68
anatom_site_general_challenge    527
diagnosis                          0
benign_malignant                   0
target                             0
dtype: int64
</code></pre></div></div> <p>We observe that the metadata contains several missing values. Imputation strategies like replacing with mean or k-nearest neighbors could be used. However, we did not go ahead with the same as we feel that it might induce some bias and negatively influence the classifier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># prepare the data: (training_images, labels)
</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">image_path</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">image_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">img_name</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">train_images_path</span><span class="p">,</span> <span class="n">img_name</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.png</span><span class="sh">'</span><span class="p">)).</span><span class="n">values</span>
<span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">image_path</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">image_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">img_name</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">test_images_path</span><span class="p">,</span> <span class="n">img_name</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.png</span><span class="sh">'</span><span class="p">)).</span><span class="n">values</span>
<span class="n">test_df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">test.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>Let‚Äôs take a look at the sample images of both classes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_images</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="n">target</span><span class="p">].</span><span class="nf">sample</span><span class="p">(</span><span class="n">nrows</span> <span class="o">*</span> <span class="n">ncols</span><span class="p">)[</span><span class="sh">'</span><span class="s">image_path</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">nrows</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">image_path</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># benign samples
</span><span class="nf">plot_images</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <div class="output-plot"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kaggle_melanoma_challenge/benign_samples-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kaggle_melanoma_challenge/benign_samples-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kaggle_melanoma_challenge/benign_samples-1400.webp"/> <img src="/assets/img/kaggle_melanoma_challenge/benign_samples.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># malign samples
</span><span class="nf">plot_images</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="output-plot"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kaggle_melanoma_challenge/malign_samples-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kaggle_melanoma_challenge/malign_samples-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kaggle_melanoma_challenge/malign_samples-1400.webp"/> <img src="/assets/img/kaggle_melanoma_challenge/malign_samples.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Let‚Äôs take a look at the distribution of target class label:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">% benign: {:.4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">% malign: {:.4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>% benign: 0.9824
% malign: 0.0176
</code></pre></div></div> <p>Upon analyzing the dataset, it is observed that</p> <ul> <li>Target class distribution is not balanced, and more samples belong to the benign (majority) class than the malign (minority) class</li> <li>If we directly split the dataset into a proportion of say 80:20, then it is possible that the split may not be representative of the actual dataset having the same ratio of the class labels</li> <li>This will induce a bias towards predicting the benign class label and thus significantly impact the performance of the classifier</li> </ul> <p>In order to avoid the bias due to an imbalanced dataset and ensure the same distribution of the class labels, we employ the stratified k-fold cross-validation to obtain the same distribution of the class labels in each fold. This cross-validation ensures that we are able to make predictions on all of the data using k different models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create folds
</span><span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">kfold</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train_df_labels</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">values</span>

<span class="n">skf</span> <span class="o">=</span> <span class="nc">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">)</span>

<span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">valid_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_df</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">train_df_labels</span><span class="p">)):</span>
    <span class="n">train_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">valid_idx</span><span class="p">,</span> <span class="sh">'</span><span class="s">kfold</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fold_idx</span>

<span class="n">train_df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">train_folds.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>Now let‚Äôs create a custom data loader to load the data from the specified image paths; it is also capable of performing transformations(if required), directly at the loading stage, so we don‚Äôt need to worry about the transformations at later stages.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MelanomaDataset</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_paths</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">resize</span><span class="p">,</span> <span class="n">augmentations</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize the Melanoma Dataset Class
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="n">self</span><span class="p">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span>
        <span class="n">self</span><span class="p">.</span><span class="n">resize</span> <span class="o">=</span> <span class="n">resize</span>
        <span class="n">self</span><span class="p">.</span><span class="n">augmentations</span> <span class="o">=</span> <span class="n">augmentations</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Returns the data instance from specified index location
        </span><span class="sh">"""</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">targets</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="c1"># open the image using PIL
</span>        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">resize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span>
                <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">resize</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">resize</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">resample</span><span class="o">=</span><span class="n">Image</span><span class="p">.</span><span class="n">BILINEAR</span>
            <span class="p">)</span>
        
        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="c1"># perform the augmentations if any
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">augmentations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">augmented</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">augmentations</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">augmented</span><span class="p">[</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">]</span>
        
        <span class="c1"># make the channel first
</span>        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">image</span><span class="p">),</span>
            <span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Returns the number of examples / instances
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>
</code></pre></div></div> <h2 id="evaluation-metrics">Evaluation Metrics</h2> <p>The area under the ROC curve (AUC) was used as an evaluation metric for the problem due to an imbalanced dataset. A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classifier at various classification thresholds. It is a measure of how well the model is capable of distinguishing between the different classes. This curve plots two parameters:</p> <ul> <li><strong>True Positive Rate (TPR)</strong> is a synonym for recall and is therefore defined as follows:</li> </ul> \[{TPR = \frac{TP}{TP + FN}}\] <ul> <li><strong>False Positive Rate (FPR)</strong> is defined as follows:</li> </ul> \[{FPR = \frac{FP}{FP + TN}}\] <p>AUC is a measure of the area underneath the entire ROC curve. It represents the degree of separability. It ranges in value from 0 to 1. The higher the AUC, the better the model is at distinguishing classes.</p> <p>For more details, please refer to the <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">Classification: ROC Curve and AUC</a> by Google‚Äôs Machine Learning Crash Course.</p> <h2 id="losses">Losses</h2> <p>We use the Binary Cross Entropy (BCE) loss for the problem since here we need to classify the images into classes: benign or malignant. The formula of the BCE loss is as given below:</p> \[L = -\frac{1}{N}\sum_{i=1}^N{(y_i\log(p_i) + (1 - y_i)\log(1 - p_i))}\] <p>where $y_i$ is the class label (0 for benign and 1 for malign) and $p_i$ is the predicted probability of the image being malign for the $i^{th}$ sample</p> <p>We will use the <code class="language-plaintext highlighter-rouge">nn.BCEWithLogitsLoss</code> directly from the PyTorch‚Äôs <code class="language-plaintext highlighter-rouge">nn</code> module.</p> <p>Another loss that we try for the problem is the Focal loss, an extension of BCE loss that tries to handle class imbalance by penalizing the misclassified examples. It is expressed as follows:</p> \[L = -\alpha_t(1 - p_t)^\gamma\log(p_t\] \[\alpha_t= \left\{\begin{matrix} \alpha &amp; if \; y = 1\\ 1 - \alpha &amp; otherwise \end{matrix}\right.\] <p>where $\gamma$ is a prefixed positive scalar value and $\alpha$ is a prefixed value between 0 and 1 to balance the positive labeled samples and negative labeled samples.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FocalLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize the Focal Loss Class
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FocalLoss</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Calculates the Focal Loss
        </span><span class="sh">"""</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BCEWithLogitsLoss</span><span class="p">()</span>
        
        <span class="n">logits</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">type_as</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logits</span><span class="p">)</span>
        
        <span class="n">focal_loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pt</span><span class="p">)</span> <span class="o">**</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">logits</span>
        
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">focal_loss</span><span class="p">)</span>
</code></pre></div></div> <h2 id="network">Network</h2> <p>Convolutional Neural Networks are very good at the task of image processing and classifications due to the following reasons:</p> <ul> <li>Require fewer parameters i.e. less complex than feed forward networks (FFNs) but are able to achieve as efficient or even better performance</li> <li>Able to identify the low-level features such as edges as well as high-level features such as objects or patterns</li> </ul> <p>Here we try the following two different network architectures:</p> <ul> <li>EfficientNet</li> <li>Squeeze and Excitation Network</li> </ul> <h3 id="efficientnet">EfficientNet</h3> <p>The EfficientNet architecture by Tan et al. focuses on scaling up the performance of traditional CNNs in terms of accuracy and at the same time, focuses on building a more computationally efficient architecture.</p> <p>How can CNNs be Scaled up?</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kaggle_melanoma_challenge/model_scaling_types-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kaggle_melanoma_challenge/model_scaling_types-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kaggle_melanoma_challenge/model_scaling_types-1400.webp"/> <img src="/assets/img/kaggle_melanoma_challenge/model_scaling_types.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Types of Model Scaling (<a href="https://arxiv.org/abs/1905.11946">image source</a>) </div> <p>Here compound scaling is the method proposed by Tan et al.</p> <p>Let‚Äôs first analyze how traditional scaling works and why each type of scaling is necessary.</p> <ul> <li> <p><strong>Width Scaling (w)</strong>: The objective of using a wider network is that wider networks are more suited to capture more fine-grained features. This is typically used in shallow networks. But the problem is that if we make the network extremely wide, the performance of the network in terms of accuracy degrades. Therefore, we need an optimum width to maintain performance.</p> </li> <li> <p><strong>Depth Scaling (d)</strong>: Theoretically, deeper neural networks tend to capture more complex features and this makes the neural network generalize well to other tasks. But practically, if we go on making the network too deep, it will increase the computational complexity and such networks will require huge training times. Also very deep neural networks suffer from vanishing/exploding gradient problems. Therefore, we need an optimum depth to achieve good performance.</p> </li> <li> <p><strong>Resolution Scaling (r)</strong>: By intuition, we can consider that if we take a high-resolution image, it would yield more fine-grained features and thus would boost the performance. Though this is true to a certain extent, we cannot assume a linear relationship between these. This is because the accuracy gain diminishes very quickly. So to a certain extent, by resolution scaling, we can improve the performance of the network.</p> </li> </ul> <p>Based on their study, the authors have considered that all these 3 factors should be considered to a certain extent and a combined scaling technique must be incorporated.</p> <p>By intuition, if we are considering a high-resolution image, naturally, we have to increase the depth and the width of the network. To validate this intuition, the authors considered a fixed-width network (w) and varied the scaling factors r and d. It was observed that the accuracy improved when high-resolution images were passed through deeper neural networks.</p> <p>The authors have proposed a scaling technique which uses a compound coefficient $\phi$ in order to scale the width, depth and resolution of the network in a uniform fashion, which is expressed as follows:</p> \[{depth: d = \alpha^\phi}\] \[{width: w = \beta^\phi}\] \[{resolution: r = \gamma^\phi}\] \[such\ that\ \alpha\cdot\beta^2\cdot\gamma^2\approx2\ and\ \alpha,\ \beta,\ \gamma\ \geq 1\] <p>where $\phi$ is a use r-specified coefficient which can control how many resources are available and $\alpha$, $\beta$, $\gamma$ controls depth, width, image resolution, respectively.</p> <p>Firstly, for B0, the authors have fixed $\phi = 1$ and have assumed that twice more resources are available and have performed a small grid search for the other parameters. The optimal values which satisfy $\alpha\cdot\beta^2\cdot\gamma^2\approx2$, were found out to be $\alpha = 1.2$, $\beta = 1.1$ and $\gamma = 1.15$.</p> <p>Later, the authors kept these values of $\alpha$, $\beta$, $\gamma$ as constant and experimented with different values of $\phi$. The authors experiment with different values of $\phi$ to produce the variants EfficientNets B1-B7.</p> <p>For more details, please refer to the <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> paper.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">variant</span><span class="o">=</span><span class="sh">'</span><span class="s">efficientnet-b2</span><span class="sh">'</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes pretrained EfficientNet model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">EfficientNet</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">variant</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">base_model</span><span class="p">.</span><span class="n">_fc</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Returns the result of forward propagation
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">base_model</span><span class="p">.</span><span class="nf">extract_features</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="c1"># loss = nn.BCEWithLogitsLoss()(out, target.view(-1, 1).type_as(out))
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nc">FocalLoss</span><span class="p">()(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">type_as</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">loss</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
</code></pre></div></div> <h3 id="squeeze-and-excitation-networks">Squeeze and Excitation Networks</h3> <p>Traditional convolutional neural networks (CNNs) use convolution operation which fuses information both spatially and channel-wise, but Jie Hu et al. proposed a novel architecture Squeeze and Excitation Networks (SENets) in the 2017 ImageNet challenge that focuses on the channel-wise information correlation. This network improved the results from the previous year by 25%.</p> <p>The basic intuition behind this approach was to adjust the feature map channel-wise by adding the parameters to each channel of a convolutional block. These parameters represent the relevance of each feature map to the information, much like we use attention in the recurrent neural networks (RNNs).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kaggle_melanoma_challenge/squeeze_and_excitation_block-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kaggle_melanoma_challenge/squeeze_and_excitation_block-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kaggle_melanoma_challenge/squeeze_and_excitation_block-1400.webp"/> <img src="/assets/img/kaggle_melanoma_challenge/squeeze_and_excitation_block.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Squeeze and Excitation Block (<a href="https://arxiv.org/abs/1709.01507">image source</a>) </div> <p>The above figure represents the Squeeze-and-Excitation (SE) block where it performs a series of operations: squeeze and excitation, which allows the network to recalibrate the channel-wise information i.e. emphasize informative feature maps and suppresses less useful feature maps. The squeeze operation produces a channel descriptor expressive of the whole image by aggregating feature maps across the spatial dimensions using global average pooling. The excitation operation produces channel-wise relevance using the two fully-connected (FC) layers where the FC captures channel-wise dependencies. This block can be directly applied to the existing architectures such as ResNet, which is shown below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kaggle_melanoma_challenge/se_residual_block-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kaggle_melanoma_challenge/se_residual_block-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kaggle_melanoma_challenge/se_residual_block-1400.webp"/> <img src="/assets/img/kaggle_melanoma_challenge/se_residual_block.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Residual module (left) and SE ResNet module (right) (<a href="https://arxiv.org/abs/1709.01507">image source</a>) </div> <p>The computational overhead of the network depends on where you apply the SE block. There was a minor increase in the computational overhead, which is feasible compared to the performance boost achieved from the network. The authors applied the SE block at earlier layers to reduce the computation overhead since, at later layers, the number of parameters increases as the feature maps increase channel-wise.</p> <p>For more details, please refer to the <a href="https://arxiv.org/pdf/1709.01507.pdf">Squeeze-and-Excitation Networks</a> paper.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes pretrained EfficientNet model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">pretrainedmodels</span><span class="p">.</span><span class="nf">se_resnext50_32x4d</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="sh">'</span><span class="s">imagenet</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Returns the result of forward propagation
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">base_model</span><span class="p">.</span><span class="nf">features</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="c1"># loss = nn.BCEWithLogitsLoss()(out, target.view(-1, 1).type_as(out))
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nc">FocalLoss</span><span class="p">()(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">type_as</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">loss</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
</code></pre></div></div> <h2 id="training-and-prediction">Training and Prediction</h2> <p>Here, we use early stopping and learning rate scheduler for training the model faster.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">fold</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Train the model on a fold
    </span><span class="sh">"""</span>

    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">train_bs</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">valid_bs</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">best_score</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">Inf</span>
    <span class="n">es_patience</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">patience</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./model_fold_{:02d}.pth</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">fold</span><span class="p">)</span>

    <span class="n">train_folds_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">train_folds_df_path</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_folds_df</span><span class="p">[</span><span class="n">train_folds_df</span><span class="p">.</span><span class="n">kfold</span> <span class="o">!=</span> <span class="n">fold</span><span class="p">].</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">valid_df</span> <span class="o">=</span> <span class="n">train_folds_df</span><span class="p">[</span><span class="n">train_folds_df</span><span class="p">.</span><span class="n">kfold</span> <span class="o">==</span> <span class="n">fold</span><span class="p">].</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="n">train_images</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="n">image_path</span><span class="p">.</span><span class="n">values</span>
    <span class="n">train_targets</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">values</span>
    <span class="n">valid_images</span> <span class="o">=</span> <span class="n">valid_df</span><span class="p">.</span><span class="n">image_path</span><span class="p">.</span><span class="n">values</span>
    <span class="n">valid_targets</span> <span class="o">=</span> <span class="n">valid_df</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">values</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">)</span>

    <span class="c1"># augmentations for train and validation images
</span>    <span class="n">train_aug</span> <span class="o">=</span> <span class="n">albumentations</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">albumentations</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">max_pixel_value</span><span class="o">=</span><span class="mf">255.0</span><span class="p">,</span> <span class="n">always_apply</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="n">albumentations</span><span class="p">.</span><span class="nc">ShiftScaleRotate</span><span class="p">(</span><span class="n">shift_limit</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span> <span class="n">scale_limit</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">rotate_limit</span><span class="o">=</span><span class="mi">15</span><span class="p">),</span>
        <span class="n">albumentations</span><span class="p">.</span><span class="nc">Flip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="p">])</span>

    <span class="n">valid_aug</span> <span class="o">=</span> <span class="n">albumentations</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">albumentations</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">max_pixel_value</span><span class="o">=</span><span class="mf">255.0</span><span class="p">,</span> <span class="n">always_apply</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="p">])</span>

    <span class="c1"># creating dataset and dataloader for train and validation images
</span>    <span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">MelanomaDataset</span><span class="p">(</span>
        <span class="n">image_paths</span><span class="o">=</span><span class="n">train_images</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="n">train_targets</span><span class="p">,</span>
        <span class="n">resize</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">augmentations</span><span class="o">=</span><span class="n">train_aug</span>
    <span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span>
    <span class="p">)</span>

    <span class="n">valid_dataset</span> <span class="o">=</span> <span class="nc">MelanomaDataset</span><span class="p">(</span>
        <span class="n">image_paths</span><span class="o">=</span><span class="n">valid_images</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="n">valid_targets</span><span class="p">,</span>
        <span class="n">resize</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">augmentations</span><span class="o">=</span><span class="n">valid_aug</span>
    <span class="p">)</span>
    <span class="n">valid_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">valid_bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span>
    <span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="nc">ReduceLROnPlateau</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">threshold</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">max</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">valid_steps</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># model in train mode
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

        <span class="n">tk0</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">tk0</span><span class="p">):</span>

                <span class="c1"># load tensor to GPU
</span>                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                    <span class="n">data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                
                <span class="c1"># forward pass
</span>                <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">)</span>

                <span class="c1"># backward pass, optimize
</span>                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

                <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">train_steps</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># update progress bar
</span>                <span class="n">tk0</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">train_loss</span><span class="o">/</span><span class="n">train_steps</span><span class="p">)</span>

        <span class="n">tk0</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

        <span class="c1"># model in eval mode
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">val_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">valid_df</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">tk0</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">valid_loader</span><span class="p">),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">tk0</span><span class="p">):</span>

                <span class="c1"># load tensor to GPU
</span>                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                    <span class="n">data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                
                <span class="c1"># model prediction
</span>                <span class="n">batch_preds</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">)</span>

                <span class="n">start</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">*</span> <span class="n">valid_bs</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">])</span>
                <span class="n">val_predictions</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_preds</span><span class="p">.</span><span class="nf">cpu</span><span class="p">()</span>
                
                <span class="n">valid_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">valid_steps</span> <span class="o">+=</span> <span class="mi">1</span>
                
                <span class="c1"># update progress bar
</span>                <span class="n">tk0</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">valid_loss</span><span class="o">/</span><span class="n">valid_steps</span><span class="p">)</span>
        
        <span class="n">tk0</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

        <span class="c1"># schedule learning rate
</span>        <span class="n">auc</span> <span class="o">=</span> <span class="nf">roc_auc_score</span><span class="p">(</span><span class="n">valid_df</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">val_predictions</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch = {} , AUC = {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">auc</span><span class="p">))</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">auc</span><span class="p">)</span>

        <span class="c1"># early stopping
</span>        <span class="k">if</span> <span class="n">best_score</span> <span class="o">&lt;</span> <span class="n">auc</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Validation score improved ({} -&gt; {}). Saving Model!</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">best_score</span><span class="p">,</span> <span class="n">auc</span><span class="p">))</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="n">auc</span>
            <span class="n">patience</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">model_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">patience</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Early stopping counter: {} out of {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="n">es_patience</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">patience</span> <span class="o">==</span> <span class="n">es_patience</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Early stopping! Best AUC: {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">best_score</span><span class="p">))</span>
                <span class="k">break</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">fold</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Model predictions on a fold
    </span><span class="sh">"""</span>

    <span class="n">test_bs</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">model_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./model_fold_{:02d}.pth</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">fold</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">test_df_path</span><span class="p">)</span>
    
    <span class="n">test_images</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">.</span><span class="n">image_path</span><span class="p">.</span><span class="n">values</span>
    <span class="n">test_targets</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">test_images</span><span class="p">))</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">)</span>
    
    <span class="c1"># test augmentation on test images
</span>    <span class="n">test_aug</span> <span class="o">=</span> <span class="n">albumentations</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">albumentations</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">max_pixel_value</span><span class="o">=</span><span class="mf">255.0</span><span class="p">,</span> <span class="n">always_apply</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="p">])</span>
    
    <span class="c1"># dataset and dataloader for test images
</span>    <span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">MelanomaDataset</span><span class="p">(</span>
        <span class="n">image_paths</span><span class="o">=</span><span class="n">test_images</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="n">test_targets</span><span class="p">,</span>
        <span class="n">resize</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">augmentations</span><span class="o">=</span><span class="n">test_aug</span>
    <span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">test_bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span>
    <span class="p">)</span>
    
    <span class="c1"># model in eval mode
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">tk0</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">tk0</span><span class="p">):</span>
            
            <span class="c1"># load tensor to GPU
</span>            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                
            <span class="n">batch_preds</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">)</span>
            
            <span class="n">start</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">*</span> <span class="n">test_bs</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">])</span>
            <span class="n">test_predictions</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_preds</span><span class="p">.</span><span class="nf">cpu</span><span class="p">()</span>
    
    <span class="n">tk0</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">test_predictions</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>

</code></pre></div></div> <p>Now, let‚Äôs train each fold and save the best model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
    <span class="nf">train</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</code></pre></div></div> <p>Great, now we are ready with our models so let‚Äôs predict the targets on the test images:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">),</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">ravel</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
    <span class="n">final_predictions</span> <span class="o">+=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="n">final_predictions</span> <span class="o">/=</span> <span class="n">n_splits</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./sample_submission.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sample</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">final_predictions</span>
<span class="n">sample</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">submission.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>Here, we had trained 2 models, SEResNeXt50_32x4d and the B2 variant of the EfficientNet model. Both models were trained using the loss functions BCE Loss and Focal Loss and the results are compared and tabulated as follows:</p> <table> <thead> <tr> <th>Model</th> <th>BCE Loss</th> <th>Focal Loss</th> </tr> </thead> <tbody> <tr> <td>SEResNeXt50_32x4d</td> <td>0.8934</td> <td>0.8762</td> </tr> <tr> <td>EfficientNet B2</td> <td>0.8972</td> <td>0.8921</td> </tr> <tr> <td>SEResNeXt50_32x4d + EfficientNet B2</td> <td>0.9019</td> <td>-</td> </tr> </tbody> </table> <p>In the 3rd case, we average out the predictions of both the models and assess the performance.</p> <h2 id="future-resources">Future Resources</h2> <p>Kaggle notebooks are a great place to learn and adapt to best practices of the experts. Here are the few kernels from the competition you can refer:</p> <ul> <li><a href="https://www.kaggle.com/nxrprime/siim-d3-eda-augmentations-and-resnext">SIIM: d3 EDA, Augmentations and ResNeXt</a></li> <li><a href="https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble">Analysis of Melanoma Metadata and EffNet Ensemble</a></li> <li><a href="https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords">Triple Stratified KFold with TFRecords</a></li> </ul> <div class="citations"> <d-cite key="skin_cancer_melanoma"> <d-cite key="siim_isic_kaggle"> <d-cite key="preprocessed_isic"> <d-cite key="roc_auc_google"> <d-cite key="lin_2022_focal_loss"> <d-cite key="Tan2019EfficientNetRM"> <d-cite key="EfficientNetRethinking"> <d-cite key="Hu2020SqueezeandExcitationN"> &lt;/div&gt; </d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></div>]]></content><author><name>Jay Gala</name></author><category term="python"/><category term="pytorch"/><category term="kaggle-competition"/><summary type="html"><![CDATA[This blog gives a gentle introduction for beginners on getting started with Kaggle competitions.]]></summary></entry><entry><title type="html">Neural Style Transfer</title><link href="https://jaygala24.github.io/blog/2020/neural-style-transfer/" rel="alternate" type="text/html" title="Neural Style Transfer"/><published>2020-07-31T00:00:00+00:00</published><updated>2020-07-31T00:00:00+00:00</updated><id>https://jaygala24.github.io/blog/2020/neural-style-transfer</id><content type="html" xml:base="https://jaygala24.github.io/blog/2020/neural-style-transfer/"><![CDATA[<p>This post assumes that you have basic skills of working with <a href="https://pytorch.org/">PyTorch</a>. If you are new to PyTorch, I would highly encourage you to go through <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep Leaning With PyTorch: A 60 Minute Blitz</a> by PyTorch. It‚Äôs a great place for beginners to get your hands dirty.</p> <p>Neural Style Transfer is an algorithm developed by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge that blends the content of one image with the style of another image using Deep Neural Networks to create artistic images of high perceptual quality.</p> <h2 id="intuition">Intuition</h2> <p>Convolutional Neural Networks are very powerful, extracting the visual information hierarchically. This makes them really useful for this task. The lower layers care more about the detailed pixel values, whereas the higher layers care more about the actual content of the image (objects such as eyes, nose, etc.).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neural_style_transfer/cnn_layer_output-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neural_style_transfer/cnn_layer_output-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neural_style_transfer/cnn_layer_output-1400.webp"/> <img src="/assets/img/neural_style_transfer/cnn_layer_output.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Convolutional Neural Network (CNN) (<a href="https://arxiv.org/abs/1508.06576">image source</a>) </div> <p>In the above figure, the output image is a mix of two since we use the activations of the neural network at specific layers as a filter to get the intermediate style and content output of the inputs.</p> <p>The principle underlying the neural style transfer is simple:</p> <ul> <li>Define two distances, one for the content and one for style.</li> <li>Measure how different the content and style are between two images, respectively.</li> <li>Reconstruct the image from white noise using backpropagation by minimizing both content and style distance with the content and style images, respectively.</li> </ul> <h2 id="losses-involved">Losses Involved</h2> <h3 id="content-loss">Content Loss</h3> \[L_{content}(\bar{p}, \bar{x}, \bar{l}) = \frac{1}{2}\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2\] <p>where,</p> <ul> <li>$\bar{p}$ and $\bar{x}$ are the content and generated images respectively.</li> <li>$F_{i,j}$ and $P_{i,j}$ are the feature representation of the original and generated image of $i^{th}$ filter at position $j$ in layer $l$ respectively.</li> </ul> <h3 id="style-loss">Style Loss</h3> \[E_{l} = \frac{1}{4N_{l}^2M_{l}^2}\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2\] \[L_{style}(\bar{a}, \bar{x}) = \sum_{i=0}^{L}w_{l}E_{l}\] <p>where,</p> <ul> <li>$\bar{a}$ and $\bar{x}$ are the style and generated image respectively.</li> <li>$A^{l}$ and $G^{l}$ are the style representation (Gram Matrix) at layer $l$ respectively.</li> <li>$w_{l}$ and $E_{l}$ are weighing factor and error for specific layer ${l}$ respectively.</li> </ul> <h3 id="total-loss">Total Loss</h3> <p>which is a weighted sum of the two above:</p> \[L_{total}(\bar{p}, \bar{a}, \bar{x}) = \alpha L_{content}(\bar{p}, \bar{x}) + \beta L_{style}(\bar{a}, \bar{x})\] <p>where,</p> <ul> <li>$L_{content}$ and $L_{style}$ are the content and style loss respectively.</li> <li>$\alpha$ and $\beta$ are weights for the content and style loss, respectively.</li> <li>$\bar{p}$, $\bar{a}$ $\bar{x}$ are the content, style and generated images respectively.</li> </ul> <p>There is a trade-off between the actual content and artistic style, which is determined by $\alpha$ and $\beta$. If the content is more important, then increase the $\alpha$. If the style is more important, then increase the $\beta$.</p> <h2 id="code-walkthrough">Code Walkthrough</h2> <p>Now, let‚Äôs go to the implementation of the above algorithm by importing the below packages.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <p>Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using <code class="language-plaintext highlighter-rouge">torch.cuda.is_available()</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># device configuration
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Now, let‚Äôs download and load the pre-trained VGG19 model. VGG is trained for the task of object detection. We freeze all VGG parameters as we are using it for optimizing the target image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load the pretrained VGG19
</span><span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">features</span>

<span class="c1"># move the vgg model to GPU in eval mode (freeze model parameters) if available
</span><span class="n">vgg</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace=True)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace=True)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace=True)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace=True)
  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (17): ReLU(inplace=True)
  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace=True)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace=True)
  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (24): ReLU(inplace=True)
  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (26): ReLU(inplace=True)
  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace=True)
  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (31): ReLU(inplace=True)
  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (33): ReLU(inplace=True)
  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (35): ReLU(inplace=True)
  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
</code></pre></div></div> <p>Now, let‚Äôs load the style and content images. The PIL images loaded have values between 0 to 255, but when they are transformed into torch tensors, their values are converted between 0 and 1. We perform few transformations such as <code class="language-plaintext highlighter-rouge">Resize()</code>, <code class="language-plaintext highlighter-rouge">ToTensor()</code>, <code class="language-plaintext highlighter-rouge">Normalize()</code> on the image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># desired size of the output image
</span><span class="n">imsize</span> <span class="o">=</span> <span class="mi">512</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="mi">128</span>  <span class="c1"># use small size if no gpu
</span>
<span class="c1"># VGG19 mean and std for each channel
</span><span class="n">cnn_normalization_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
<span class="n">cnn_normalization_std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="c1"># scale imported image
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="n">imsize</span><span class="p">),</span>
    <span class="c1"># transform it into a torch tensor
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="c1"># normalize the tensor as per VGG network
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">cnn_normalization_mean</span><span class="p">,</span> 
                             <span class="n">std</span><span class="o">=</span><span class="n">cnn_normalization_std</span><span class="p">)</span>
<span class="p">])</span>

<span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Load an image and comvert it to a torch tensor.
    </span><span class="sh">"""</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">transform</span><span class="p">:</span>
        <span class="c1"># transform the image
</span>        <span class="n">image</span> <span class="o">=</span> <span class="nf">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="c1"># add a fake batch dimension to fit network's input dimension
</span>        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load the style and content image
</span><span class="n">style_img</span> <span class="o">=</span> <span class="nf">load_image</span><span class="p">(</span><span class="sh">'</span><span class="s">./images/style.jpg</span><span class="sh">'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">loader</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">content_img</span> <span class="o">=</span> <span class="nf">load_image</span><span class="p">(</span><span class="sh">'</span><span class="s">./images/content.jpg</span><span class="sh">'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">loader</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>Now, let‚Äôs create a function to denormalize the image tensors, which will be later helpful to display the image tensors.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">denorm_image</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Denormalize the image for visualization
    </span><span class="sh">"""</span>
    <span class="c1"># clone the image tensor and detach from tracking
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
    
    <span class="c1"># remove the fake batch dimension
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">()</span>   
    <span class="c1"># reshape (n_C, n_H, n_W) -&gt; (n_H, n_W, n_C)
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># denormalize the image
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">*</span> <span class="n">cnn_normalization_std</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">cnn_normalization_mean</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    
    <span class="c1"># restrict the value between 0 and 1 by clipping the outliers
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></div> <p>Now, let‚Äôs display the content and style image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">denorm_image</span><span class="p">(</span><span class="n">content_img</span><span class="p">))</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Content</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">denorm_image</span><span class="p">(</span><span class="n">style_img</span><span class="p">))</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Style</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neural_style_transfer/content_and_style_plot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neural_style_transfer/content_and_style_plot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neural_style_transfer/content_and_style_plot-1400.webp"/> <img src="/assets/img/neural_style_transfer/content_and_style_plot.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now, let‚Äôs select the convolutional layers from VGG19 to extract the feature maps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Extract the convolutional feature maps from conv1_1 ~ conv5_1
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># layer number for conv1_1 ~ conv5_1
</span>        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">10</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">19</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">28</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># conv feature map
</span>    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">image</span>
    
    <span class="c1"># iterate through the model layers
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">_modules</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># checks for the layer match
</span>        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">features</span>
</code></pre></div></div> <p>Now, let‚Äôs define the gram matrix used in style loss, which we try to minimize during the backpropagation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gram_matrix</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculates the gram matrix for input
    </span><span class="sh">"""</span>
    <span class="c1"># a = 1 (batch_size), b = n_C (number of feature maps)
</span>    <span class="c1"># (c, d) = dimension of feature map 
</span>    <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>

    <span class="c1"># reshape the convolutional feature maps
</span>    <span class="n">features</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>

    <span class="c1"># compute the gram matrix
</span>    <span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="p">.</span><span class="nf">t</span><span class="p">())</span>

    <span class="c1"># Normalize the values of gram matrix
</span>    <span class="n">G</span> <span class="o">=</span> <span class="n">G</span><span class="p">.</span><span class="nf">div</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">G</span>
</code></pre></div></div> <p>Now, let‚Äôs create a clone of the content image as a starting image for the target, which we transform such that the content image has an artistic style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_img</span> <span class="o">=</span> <span class="n">content_img</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Alternative way: you can start with white noise to get an image with 
# content attributes of content image and style attributes of style image
# target_img = torch.randn(content_img.data.size()).to(device)
</span></code></pre></div></div> <p>Now let‚Äôs run the model and try to minimize the loss using backpropagation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_neural_style_transfer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">content_img</span><span class="p">,</span> <span class="n">style_img</span><span class="p">,</span> <span class="n">target_img</span><span class="p">,</span> 
                              <span class="n">num_steps</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">sample_steps</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
                              <span class="n">style_weight</span><span class="o">=</span><span class="mf">1e4</span><span class="p">,</span> <span class="n">content_weight</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Run the neural style transfer
    </span><span class="sh">"""</span>
    <span class="c1"># optimizer for reconstruction of content image with artistic style
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">target_img</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">()],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> 
                                 <span class="n">betas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        
        <span class="c1"># extract the conv feature maps for target, content and style images
</span>        <span class="n">target_features</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">target_img</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">content_features</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">content_img</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">style_features</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">style_img</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    
        <span class="c1"># initialize the style and content loss
</span>        <span class="n">style_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">content_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># calculate the style and content loss for each specific layer
</span>        <span class="k">for</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">f3</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">target_features</span><span class="p">,</span> <span class="n">content_features</span><span class="p">,</span> <span class="n">style_features</span><span class="p">):</span>

            <span class="c1"># compute content loss with target and content images
</span>            <span class="n">content_loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

            <span class="c1"># compute the gram matrix for target and style feature maps
</span>            <span class="n">f1</span> <span class="o">=</span> <span class="nf">gram_matrix</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>
            <span class="n">f3</span> <span class="o">=</span> <span class="nf">gram_matrix</span><span class="p">(</span><span class="n">f3</span><span class="p">)</span>

            <span class="c1"># compute the style loss with target and style images
</span>            <span class="n">style_loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f3</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># compute total loss, backprop and optimize
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">content_loss</span> <span class="o">*</span> <span class="n">content_weight</span> <span class="o">+</span> <span class="n">style_loss</span> <span class="o">*</span> <span class="n">style_weight</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
        <span class="nf">if </span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">sample_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print the model stats
</span>            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">run {}:</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Style Loss : {:4f} Content Loss: {:4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                    <span class="n">style_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">content_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>
            <span class="nf">print</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">target_img</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># run the neural style transfer
</span><span class="n">output_img</span> <span class="o">=</span> <span class="nf">run_neural_style_transfer</span><span class="p">(</span><span class="n">vgg</span><span class="p">,</span> <span class="n">content_img</span><span class="p">,</span> <span class="n">style_img</span><span class="p">,</span> <span class="n">target_img</span><span class="p">)</span>

<span class="c1"># display the style transfered image
</span><span class="n">output_img</span> <span class="o">=</span> <span class="nf">denorm_image</span><span class="p">(</span><span class="n">output_img</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">output_img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Output Image</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>run 400:
Style Loss : 0.000006 Content Loss: 21.730204

run 800:
Style Loss : 0.000004 Content Loss: 19.803923

run 1200:
Style Loss : 0.000004 Content Loss: 19.264122

run 1600:
Style Loss : 0.000004 Content Loss: 19.008064

run 2000:
Style Loss : 0.000004 Content Loss: 18.843626
</code></pre></div></div> <div class="output-plot"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neural_style_transfer/inference-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neural_style_transfer/inference-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neural_style_transfer/inference-1400.webp"/> <img src="/assets/img/neural_style_transfer/inference.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Great! Now you have become an artist who can generate artworks from a content image and style image.</p> <h2 id="future-resources">Future Resources</h2> <ul> <li>Try the <a href="https://github.com/pytorch/examples/tree/master/fast_neural_style">Fast Neural Style Transfer</a>, which uses <a href="https://arxiv.org/abs/1603.08155">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a> along with <a href="https://arxiv.org/pdf/1607.08022.pdf">Instance Normalization</a>.</li> <li>Try the <a href="https://www.tensorflow.org/tutorials/generative/style_transfer">Tensorflow Implementation</a> of Neural Style Transfer.</li> </ul> <div class="citations"> <d-cite key="gatys_2015_style_transfer"> <d-cite key="jacq_2021_style_transfer_pytorch"> &lt;/div&gt; </d-cite></d-cite></div>]]></content><author><name>Jay Gala</name></author><category term="python"/><category term="pytorch"/><category term="neural-style"/><summary type="html"><![CDATA[This blog walks through the intuition behind the neural style transfer and its implementation.]]></summary></entry><entry><title type="html">Guide to NumPy For Scientific Computing</title><link href="https://jaygala24.github.io/blog/2020/guide-to-numpy/" rel="alternate" type="text/html" title="Guide to NumPy For Scientific Computing"/><published>2020-06-20T00:00:00+00:00</published><updated>2020-06-20T00:00:00+00:00</updated><id>https://jaygala24.github.io/blog/2020/guide-to-numpy</id><content type="html" xml:base="https://jaygala24.github.io/blog/2020/guide-to-numpy/"><![CDATA[<p>This post assumes that you have the necessary skills to work with Python. If you are new to Python, I would highly encourage you to go through <a href="https://developers.google.com/edu/python">Google‚Äôs Python Class</a> first. It‚Äôs an excellent place for beginners and also offers exercises to get your hands dirty.</p> <p>Python is a great general-purpose programming language, but it becomes really convenient and powerful for Machine Learning and Data Science with a few popular libraries. The libraries provide efficient code optimization and memory management, along with some additional features and functionalities.</p> <h2 id="introduction">Introduction</h2> <p>Although all the computations can be done by Python on its own stand-alone, these libraries provide a much efficient way of doing the computations. Let‚Äôs go through an example to understand the importance of these libraries.</p> <p>Create a list of numbers from 1 to 5 and multiply the elements by 2:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="mi">2</span>
<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>  <span class="c1"># Notice something different
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]
</code></pre></div></div> <p>Multiplying the list by x concatenates the elements of the list x times. We‚Äôll have to write a function for multiplying the elements of the list by 2.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multiply_2</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Multiply elements of list by 2
    Parameters:
    arr - list
          Input list
    </span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="n">res</span> <span class="o">=</span> <span class="nf">multiply_2</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[2, 4, 6, 8, 10]
</code></pre></div></div> <p>What‚Äôs the problem with the above computation?</p> <p>This seems fine if we are working on data in smaller quantities, but we work with data in larger quantities, this approach becomes really inefficient. This is where these powerful libraries come in to aid us.</p> <p>Don‚Äôt worry if you don‚Äôt get the syntax initially; you‚Äôll get it as we progress through the post.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load the numpy library
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mi">2</span>
<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 2 3 4 5]
[ 2  4  6  8 10]
</code></pre></div></div> <p>This library uses a vectorized approach, which simply means applying the operations on whole arrays instead of individual elements. This vectorized code is highly optimized and written in C.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">timeit</span>
<span class="nf">multiply_2</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The slowest run took 4.88 times longer than the fastest. This could mean that an intermediate result is being cached.
77.5 ¬µs ¬± 33 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">timeit</span>
<span class="n">a</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>995 ns ¬± 292 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each)
</code></pre></div></div> <p>The time of execution when using these libraries is really short compared to regular python. There are many more reasons that drive us towards the usage of these libraries. But let‚Äôs get started with the usage of these libraries.</p> <h2 id="basics">Basics</h2> <p><a href="http://www.numpy.org/">Numpy</a> is the core library for doing scientific computing in Python involves working the multidimensional arrays. Many libraries are built on top of NumPy.</p> <p>A NumPy array is a grid of values, all of the same type. Nested python lists can be used to initialize the array. You can access elements with square brackets for the 1D array, but for the 2D array, it‚Äôs a little different.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load the library using np as alias
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a 1D array
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># Create a 2D array
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 2 3 4]
[[1 2]
 [3 4]]
</code></pre></div></div> <p>Let‚Äôs write a function which gives us more details of these NumPy arrays:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_info</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Prints details of the numpy array
    Parameters:
    arr - nd array
          Input array
    </span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">number of elements:</span><span class="sh">'</span><span class="p">,</span> <span class="n">arr</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">number of dimensions:</span><span class="sh">'</span><span class="p">,</span> <span class="n">arr</span><span class="p">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">shape:</span><span class="sh">'</span><span class="p">,</span> <span class="n">arr</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">data type:</span><span class="sh">'</span><span class="p">,</span> <span class="n">arr</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">strides:</span><span class="sh">'</span><span class="p">,</span> <span class="n">arr</span><span class="p">.</span><span class="n">strides</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">flags:</span><span class="sh">'</span><span class="p">,</span> <span class="n">arr</span><span class="p">.</span><span class="n">flags</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">ndarray.size</code>: Tells us about the number of elements in a NumPy array.</li> <li><code class="language-plaintext highlighter-rouge">ndarray.ndim</code>: Tells us about the number of dimensions in a NumPy array.</li> <li><code class="language-plaintext highlighter-rouge">ndarray.shape</code>: Tells us about the size of the NumPy array along each dimension.</li> <li><code class="language-plaintext highlighter-rouge">ndarray.dtype</code>: Tells us about the data type of elements in a NumPy array.</li> <li><code class="language-plaintext highlighter-rouge">ndarray.strides</code>: Tells us about the no. of bytes need to step in each dimension to access the adjacent element. Strides will be multiples of 8 along each dimension.</li> <li><code class="language-plaintext highlighter-rouge">ndarray.flags</code>: Tells us about how the NumPy array is stored in memory. <code class="language-plaintext highlighter-rouge">C_CONTIGUOUS</code> tells us that elements in the memory are row-wise. <code class="language-plaintext highlighter-rouge">F_CONTIGUOUS</code> tells us that elements in the memory are column-wise.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 4
number of dimensions: 1
shape: (4,)
data type: int64
strides: (8,)
flags:   C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 4
number of dimensions: 2
shape: (2, 2)
data type: int64
strides: (16, 8)
flags:   C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <p>Numpy arrays are referenced based. They point to the same array in memory when you assign a defined array to another variable. So you should be careful that you don‚Äôt modify the information in place, which may be useful for later purposes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a:</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">b:</span><span class="sh">'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: [0 1 2 3 4]
b: [5 1 2 3 4]
</code></pre></div></div> <p>Numpy provides a <code class="language-plaintext highlighter-rouge">copy</code> method to create a copy of the same array in memory.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a:</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">b:</span><span class="sh">'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: [0 1 2 3 4]
b: [0 1 2 3 4]
</code></pre></div></div> <p>Numpy also provides many functions to create arrays. Most of the functions take shape as a parameter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of all ones
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a:</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="c1"># Create an array of all zeros
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">b:</span><span class="sh">'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Create a constant array
</span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>          
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">c:</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="c1"># Create an identity matrix of 3x3
</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>                   
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">d:</span><span class="sh">'</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="c1"># Create an array of random values
</span><span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>    
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">e:</span><span class="sh">'</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

<span class="c1"># Create an array of random values from uniform distribution
</span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">f:</span><span class="sh">'</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

<span class="c1"># Create an array of random values from normal distribution
</span><span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">g:</span><span class="sh">'</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: [[1. 1. 1.]]
b: [[0. 0. 0.]]
c: [[4 4 4]]
d: [[1. 0. 0.]
    [0. 1. 0.]
    [0. 0. 1.]]
e: [[0.22551855 0.18974127 0.87953329]]
f: [0.54819848 0.01511915 0.5365319 ]
g: [-0.87897997 -1.95487666  0.62275   ]
</code></pre></div></div> <p>Numpy array provides different numeric datatypes options to construct the arrays. This can be really useful when you have a large dataset, so you can set the datatype based on the data limits to be memory efficient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Numpy chooses the datatype
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Datatype of a:</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Datatype of b:</span><span class="sh">'</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Explicitly specify the datatype
</span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Datatype of c:</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Datatype of d:</span><span class="sh">'</span><span class="p">,</span> <span class="n">d</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Datatype of a: int64
Datatype of b: float64
Datatype of c: int32
Datatype of d: float32
</code></pre></div></div> <h2 id="slicing">Slicing</h2> <p>Although slicing is similar to Python lists, there‚Äôs a slight difference for multi-dimensional arrays. We need to <code class="language-plaintext highlighter-rouge">specify the slice for each dimension</code> of the array. Let‚Äôs walk through some examples to get a better understanding.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (4, 4)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Slice the first 2 rows from array a
</span><span class="n">a</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span>    <span class="c1"># same as a[:2]
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0, 1, 2, 3],
       [4, 5, 6, 7]])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Slice the first 2 rows and first 2 cols from array a
</span><span class="n">a</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0, 1],
       [4, 5]])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Slice the alternate rows and alternate cols from array a
</span><span class="n">a</span><span class="p">[::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 0,  2],
       [ 8, 10]])
</code></pre></div></div> <p>Now try slicing the four center elements of the array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 5,  6],
       [ 9, 10]])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Alternative way to extract the same elements but the dimension would be reduced to one
</span><span class="n">a</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 5,  6,  9, 10])
</code></pre></div></div> <p>Now try accessing the elements 1, 2, 4, 7 of the array such that the dimension is not reduced:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">[[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[1, 2],
       [4, 7]])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Alternative way for getting the above result
</span><span class="n">a</span><span class="p">[[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[1, 2],
       [4, 7]])
</code></pre></div></div> <p>Note that the alternative way uses broadcasting, which will be discussed later in the post. Things will be much clearer there, so please have a bit of patience.</p> <h2 id="functions-and-aggregations">Functions and Aggregations</h2> <p>Numpy comes with a lot of built-in functions, which are useful for performing various computations efficiently. One can perform arithmetic, matrix, trigonometric, exponent, logarithm operations, and many more. Let‚Äôs go through a few of these operations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a:</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">b:</span><span class="sh">'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: [[0 1]
    [2 3]]
b: [[1. 1.]
    [1. 1.]]
</code></pre></div></div> <h3 id="arithmetic-operations">Arithmetic Operations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Elementwise sum
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a + b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>                 <span class="c1"># same as print(a + b)
</span>
<span class="c1"># Elementwise difference
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a - b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>     <span class="c1"># same as print(a - b)
</span>
<span class="c1"># Elementwise product
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a * b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>        <span class="c1"># same as print(a * b)
</span>
<span class="c1"># Elementwise division
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a / b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>         <span class="c1"># same as print(a / b)
</span>
<span class="c1"># Elementwise modulo
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a % b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">mod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>              <span class="c1"># same as print(a % b)
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a + b = [[1. 2.]
         [3. 4.]]
a - b = [[-1.  0.]
         [ 1.  2.]]
a * b = [[0. 1.]
         [2. 3.]]
a / b = [[0. 1.]
         [2. 3.]]
a % b = [[0. 0.]
         [0. 0.]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, 3)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>

<span class="c1"># Absolute values
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">|a| =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>            <span class="c1"># same as print(np.absolute(a))
</span>
<span class="c1"># Square values
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a ^ 2 =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>      <span class="c1"># same as print(a ** 2)
</span>
<span class="c1"># Square root values
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a ^ 0.5 =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">a</span><span class="p">)))</span>  <span class="c1"># same as print(np.abs(a) ** 0.5)
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|a| = [[0 1 2]
       [3 4 5]
       [6 7 8]]
a ^ 2 = [[ 0  1  4]
         [ 9 16 25]
         [36 49 64]]
a ^ 0.5 = [[0.         1.         1.41421356]
           [1.73205081 2.         2.23606798]
           [2.44948974 2.64575131 2.82842712]]
</code></pre></div></div> <h3 id="matrix-operations">Matrix Operations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a matrix of shape (2, 2)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>

<span class="c1"># Create a matrix of shape (2,)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Dot product of vector / vector
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">b x b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>     <span class="c1"># same as print(b.dot(b))
</span>
<span class="c1"># Product of matrix / vector
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a x b =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>     <span class="c1"># same as print(a.dot(b))
</span>
<span class="c1"># Product of matrix / matrix
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a x a =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>     <span class="c1"># same as print(a.dot(a))
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>b x b = 20
a x b = [ 4 16]
a x a = [[ 2  3]
         [ 6 11]]
</code></pre></div></div> <h3 id="trigonometric-operations">Trigonometric Operations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create array of equal inerval values from 0 to pi radians
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">theta =</span><span class="sh">'</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">sin(theta) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cos(theta) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">tan(theta) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">tan</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>theta = [0.         1.57079633 3.14159265]
sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16]
cos(theta) = [ 1.000000e+00  6.123234e-17 -1.000000e+00]
tan(theta) = [ 0.00000000e+00  1.63312394e+16 -1.22464680e-16]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, )
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">x =</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">arcsin(x) =</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">arcsin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">arccos(x) =</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">arccos</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">arctan(x) =</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">arctan</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = [-1  0  1]
arcsin(x) = [-1.57079633  0.          1.57079633]
arccos(x) = [3.14159265 1.57079633 0.        ]
arctan(x) = [-0.78539816  0.          0.78539816]
</code></pre></div></div> <h3 id="exponentiation-operations">Exponentiation Operations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (4,)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a =</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">e ^ a =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">e ^ a - 1 =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">expm1</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">2 ^ a =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">10 ^ a =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = [1 2 3 4]
e ^ a = [ 2.71828183  7.3890561  20.08553692 54.59815003]
e ^ a - 1 = [ 1.71828183  6.3890561  19.08553692 53.59815003]
2 ^ a = [ 2.  4.  8. 16.]
10 ^ a = [   10   100  1000 10000]
</code></pre></div></div> <h3 id="logarithmic-operations">Logarithmic Operations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (4,)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a =</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">ln(a) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">ln(a + 1) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">log2(a) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">log2</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">log10(a) =</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = [1 2 3 4]
ln(a) = [0.         0.69314718 1.09861229 1.38629436]
ln(a + 1) = [0.69314718 1.09861229 1.38629436 1.60943791]
log2(a) = [0.        1.        1.5849625 2.       ]
log10(a) = [0.         0.30103    0.47712125 0.60205999]
</code></pre></div></div> <h3 id="aggregates">Aggregates</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (4,)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 2 3 4]
</code></pre></div></div> <p>Calling the <code class="language-plaintext highlighter-rouge">reduce</code> method on arithmetic functions like <code class="language-plaintext highlighter-rouge">add</code> returns the sum of all elements in the array. Similarly, calling the <code class="language-plaintext highlighter-rouge">accumulate</code> method on arithmetic functions like <code class="language-plaintext highlighter-rouge">add</code> returns the array of intermediate results:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sum of all elements of array
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">.</span><span class="nf">reduce</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="c1"># Product of all elements of array
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">.</span><span class="nf">reduce</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="c1"># Intermediate result of sum
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">.</span><span class="nf">accumulate</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="c1"># Intermediate result of product
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">.</span><span class="nf">accumulate</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10
24
[ 1  3  6 10]
[ 1  2  6 24]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (2, 2)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.68347343 0.56920798]
 [0.20602274 0.92199748]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sum of all elements of array
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>    <span class="c1"># same as print(a.sum())
</span>
<span class="c1"># Minimum value of array
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>    <span class="c1"># same as print(a.min())
</span>
<span class="c1"># Maximum value of array
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>    <span class="c1"># same as print(a.max())
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2.380701625644927
0.2060227376885182
0.9219974808291727
</code></pre></div></div> <p>We can find the <code class="language-plaintext highlighter-rouge">sum</code>, <code class="language-plaintext highlighter-rouge">min</code>, and <code class="language-plaintext highlighter-rouge">max</code> row-wise or col-wise by specifying the <code class="language-plaintext highlighter-rouge">axis</code> argument.</p> <ul> <li><code class="language-plaintext highlighter-rouge">axis = 0</code> specifies we are reducing rows that means we are finding row-wise</li> <li><code class="language-plaintext highlighter-rouge">axis = 1</code> specifies we are reducing cols that means we are finding col-wise</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sum of all elements of array row wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>    <span class="c1"># same as print(a.sum(axis=0))
</span>
<span class="c1"># Min value of array row wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>    <span class="c1"># same as print(a.min(axis=0))
</span>
<span class="c1"># Max value of array row wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>    <span class="c1"># same as print(a.max(axis=0))
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0.88949617 1.49120546]
[0.20602274 0.56920798]
[0.68347343 0.92199748]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sum of all elements of array col wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># same as print(a.sum(axis=1))
</span>
<span class="c1"># Min value of array col wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># same as print(a.min(axis=1))
</span>
<span class="c1"># Max value of array col wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># same as print(a.max(axis=1))
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1.25268141 1.12802022]
[0.56920798 0.20602274]
[0.68347343 0.92199748]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Find the index of max and min value
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2
3
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Find the index of max and min value row wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 0]
[0 1]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Find the index of max and min value col wise
</span><span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 0]
[0 1]
</code></pre></div></div> <p>There are various other functions such as <code class="language-plaintext highlighter-rouge">np.mean</code>, <code class="language-plaintext highlighter-rouge">np.std</code>, <code class="language-plaintext highlighter-rouge">np.median</code>, <code class="language-plaintext highlighter-rouge">np.percentile</code>, <code class="language-plaintext highlighter-rouge">np.any</code>, <code class="language-plaintext highlighter-rouge">np.all</code> which you can refer in the documentation.</p> <h2 id="broadcasting">Broadcasting</h2> <p>Broadcasting is a way that allows us to work with NumPy arrays of different shapes when performing arithmetic operations.</p> <p><strong>Rules of Broadcasting:</strong></p> <ul> <li>Array with fewer dimensions is padded with ones on the leading side</li> <li>If the shape of two arrays do not match in any dimension, the array with a shape equal to 1 is stretched to match the other shape</li> <li>If in any dimension the sizes disagree and neither equal to 1, then the array is not compatible, thus leading to an error.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (4,)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0 1 2 3]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">+</span> <span class="mi">4</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([4, 5, 6, 7])
</code></pre></div></div> <p>In the above example, there is a duplication of the scalar value 4 into the array of shape same as an array a and performs addition, which can be demonstrated in the below figure.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/guide_to_numpy/broadcasting_scalar-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/guide_to_numpy/broadcasting_scalar-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/guide_to_numpy/broadcasting_scalar-1400.webp"/> <img src="/assets/img/guide_to_numpy/broadcasting_scalar.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (4,)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a:</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="c1"># Create an array of shape (4, 1)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">b:</span><span class="sh">'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: [0 1 2 3]
b: [[0]
 [1]
 [2]
 [3]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0, 1, 2, 3],
       [1, 2, 3, 4],
       [2, 3, 4, 5],
       [3, 4, 5, 6]])
</code></pre></div></div> <p>In the above example, a and b both stretches row-wise and col-wise respectively, which can be demonstrated in the below figure</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/guide_to_numpy/broadcasting_both_dimensions-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/guide_to_numpy/broadcasting_both_dimensions-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/guide_to_numpy/broadcasting_both_dimensions-1400.webp"/> <img src="/assets/img/guide_to_numpy/broadcasting_both_dimensions.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, 2)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create an array of shape (2,)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[2., 3.],
       [4., 5.],
       [6., 7.]])
</code></pre></div></div> <p>In the above example, broadcasting takes for the b, which is demonstrated in the below figure.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/guide_to_numpy/broadcasting_one_dimension-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/guide_to_numpy/broadcasting_one_dimension-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/guide_to_numpy/broadcasting_one_dimension-1400.webp"/> <img src="/assets/img/guide_to_numpy/broadcasting_one_dimension.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, 2)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create an array of shape (3,)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,))</span>

<span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-48-bd58363a63fc&gt; in &lt;module&gt;
----&gt; 1 a + b

ValueError: operands could not be broadcast together with shapes (3,2) (3,) 
</code></pre></div></div> <p>In the above example, broadcasting does not take place as the shapes are incompatible, which can be demonstrated in the below figure.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/guide_to_numpy/broadcasting_error-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/guide_to_numpy/broadcasting_error-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/guide_to_numpy/broadcasting_error-1400.webp"/> <img src="/assets/img/guide_to_numpy/broadcasting_error.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="fancy-indexing">Fancy Indexing</h2> <p>Fancy indexing is simply accessing multiple elements of an array at once through the use of an array of indices. It‚Äôs slicing in simple ways. Let‚Äôs walk through a few examples to understand this concept.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, 3)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[4 0 1]
 [5 2 8]
 [9 8 3]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Access the element 2, 5, 4
</span><span class="n">row</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">col</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0, 5, 3])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Access elements in random order from last 2 rows
</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="n">col</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[2, 5, 8],
       [8, 9, 3]])
</code></pre></div></div> <p>In the above example, broadcasting is used for the row indices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[20 21 29 30 28 12 10  3 38  1]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mask</span> <span class="o">=</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="mi">25</span>
<span class="nf">print</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[False False  True  True  True False False False  True False]
</code></pre></div></div> <p>Mask means boolean indexing where the <code class="language-plaintext highlighter-rouge">True</code> value indicates that the element at a particular index satisfies the conditions. This mask array helps us filtering the elements, and we can access the elements from the array using the mask as an array of indices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Array of elements greater than 25
</span><span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[29 30 28 38]
</code></pre></div></div> <p>This masking can also be useful for modifying the values of arrays that do not satisfy the constraints. Let‚Äôs see an example.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (10,)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 6  4  9  9 -7  3 -8  9  5 -3]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Elements with the negative value
</span><span class="n">neg_mask</span> <span class="o">=</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">0</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">neg_mask</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[-7 -8 -3]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Modifying the array so all elements are greater than or equal to zero
</span><span class="n">a</span><span class="p">[</span><span class="n">neg_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[6 4 9 9 0 3 0 9 5 0]
</code></pre></div></div> <h2 id="strides">Strides</h2> <p>Computer Memory is a single tape where we need to travel sequentially in order to access the data. Strides specify the number of bytes we need to travel in the memory to access the adjacent element along each dimension. Numpy arrays are stored in a contiguous block of memory, so strides become really handy. Strides are multiples of 8 and by default row-major.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, 2)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.07495098 0.03693228]
 [0.48777201 0.60112812]
 [0.47782039 0.18343593]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 6
number of dimensions: 2
shape: (3, 2)
data type: float64
strides: (16, 8)
flags:   C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">strides: (16, 8)</code> indicates that we need to travel 16 bytes to get the adjacent element row-wise and 8 bytes to get the adjacent element col-wise.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Performing the transpose of array a
</span><span class="n">a_T</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">T</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a_T</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.07495098 0.48777201 0.47782039]
 [0.03693228 0.60112812 0.18343593]]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">a_T</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 6
number of dimensions: 2
shape: (2, 3)
data type: float64
strides: (8, 16)
flags:   C_CONTIGUOUS : False
  F_CONTIGUOUS : True
  OWNDATA : False
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <p>If you notice the strides have been reversed from <code class="language-plaintext highlighter-rouge">(16, 8)</code> to <code class="language-plaintext highlighter-rouge">(8, 16)</code>, which tells us that this is just the view of array `a‚Äô. This is the reason why NumPy is memory efficient as it points to the memory instead of creating another array as compared to python.</p> <p>Note that the <code class="language-plaintext highlighter-rouge">F_CONTIGUOUS: True</code> which means the array <code class="language-plaintext highlighter-rouge">a_T</code> is column-major.</p> <p><code class="language-plaintext highlighter-rouge">np.reshape</code> is also based on the idea of strides, which returns the view of the same array with modified strides:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0.07495098 0.03693228 0.48777201 0.60112812 0.47782039 0.18343593]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 6
number of dimensions: 1
shape: (6,)
data type: float64
strides: (8,)
flags:   C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : False
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <p>Broadcasting also uses the concept of <code class="language-plaintext highlighter-rouge">strides</code> when performing arithmetic operations. Let‚Äôs see an example.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an array of shape (3, 2)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># Create an array of shape (10,)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.76096701 0.23538774]
 [0.31361445 0.58805459]
 [0.91557476 0.24585326]]
[9 0 8 2]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-65-1674f6151070&gt; in &lt;module&gt;
----&gt; 1 c = a + b

ValueError: operands could not be broadcast together with shapes (3,2) (4,) 
</code></pre></div></div> <p>Broadcasting, by default, gives us an error as the above operation does not satisfy broadcasting rules. Let‚Äôs apply some modifications, so it satisfies the broadcasting rules, and we get the array of shape <code class="language-plaintext highlighter-rouge">(4, 3, 2)</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Increase the dimension of b
</span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="nf">print_info</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 24
number of dimensions: 3
shape: (4, 3, 2)
data type: float64
strides: (48, 16, 8)
flags:   C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">np.broadcast_arrays</code> returns the broadcasted arrays of the same shape that NumPy adds together in the above code block.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a_broadcast</span><span class="p">,</span> <span class="n">b_broadcast</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">broadcast_arrays</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">a_broadcast</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 24
number of dimensions: 3
shape: (4, 3, 2)
data type: float64
strides: (0, 16, 8)
flags:   C_CONTIGUOUS : False
  F_CONTIGUOUS : False
  OWNDATA : False
  WRITEABLE : True  (with WARN_ON_WRITE=True)
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">a_broadcast</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[[0.76096701 0.23538774]
  [0.31361445 0.58805459]
  [0.91557476 0.24585326]]

 [[0.76096701 0.23538774]
  [0.31361445 0.58805459]
  [0.91557476 0.24585326]]

 [[0.76096701 0.23538774]
  [0.31361445 0.58805459]
  [0.91557476 0.24585326]]

 [[0.76096701 0.23538774]
  [0.31361445 0.58805459]
  [0.91557476 0.24585326]]]
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">strides: (0, 16, 8)</code> indicates that the virtual view of the array a in which the view of the array appears as many times as the leading shape. In the above case, it is 4 times.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print_info</span><span class="p">(</span><span class="n">b_broadcast</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number of elements: 24
number of dimensions: 3
shape: (4, 3, 2)
data type: int64
strides: (8, 0, 0)
flags:   C_CONTIGUOUS : False
  F_CONTIGUOUS : False
  OWNDATA : False
  WRITEABLE : True  (with WARN_ON_WRITE=True)
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">b_broadcast</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[[9 9]
  [9 9]
  [9 9]]

 [[0 0]
  [0 0]
  [0 0]]

 [[8 8]
  [8 8]
  [8 8]]

 [[2 2]
  [2 2]
  [2 2]]]
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">strides: (8, 0, 0)</code> indicates that the virtual array consists of every element of array b, which appears as a 2D array of shape (3, 2).</p> <p>How can we manipulate the strides and shape of an array to produce a virtual array that is much bigger but using the same memory?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Produce a virtual array which is n times bigger 
    than arr without extra memory usage
    Parameters:
    arr - nd array
          Input array
    n   - int
          Size of repeated array
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">stride_tricks</span><span class="p">.</span><span class="nf">as_strided</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span>
                                           <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="n">arr</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span>
                                           <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">arr</span><span class="p">.</span><span class="n">strides</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">repeat</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0.27505351, 0.06159749, 0.03617048, 0.33832092],
       [0.27505351, 0.06159749, 0.03617048, 0.33832092],
       [0.27505351, 0.06159749, 0.03617048, 0.33832092]])
</code></pre></div></div> <h2 id="exercises">Exercises</h2> <p>1. Create a 2D array with 1 on the border and 0 inside.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Alternative way using np.zeros
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">X</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></div> <p>2. Normalize a 5 x 5 random matrix.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">X_std</span>
</code></pre></div></div> <p>3. Given a 1D array, negate all elements which are between 3 and 8 inclusive, in place.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="n">X</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">X</span> <span class="o">&lt;</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div> <p>4. How to get the alternates dates corresponding to the month of July 2016?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="sh">'</span><span class="s">2016-07</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">2016-08</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">datetime64[D]</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>5. Create a vector of size 10 with values ranging from 0 to 1, both excluded.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Alternative way
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
</code></pre></div></div> <p>6. How to sort an array by the nth column?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">nth_col_sort_idx</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">argsort</span><span class="p">()</span>
<span class="n">X_sort</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">nth_col_sort_idx</span><span class="p">]</span>
</code></pre></div></div> <p>7. Find the nearest value from a given value in an array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">val</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">nearest</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">val</span><span class="p">).</span><span class="nf">argmin</span><span class="p">()]</span>
</code></pre></div></div> <p>8. How to accumulate elements of a vector (X) to an array (F) based on an index list (I)?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">bincount</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <p>9. Considering a four dimensions array, how to get sum over the last two axis at once?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="c1"># Flatten the two dimensions into one
</span><span class="n">new_shape</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Alternative way
# X = np.random.random((4, 4, 4, 4))
# Tuple of axis (supported for numpy 1.7.0 onwards)
# Y = X.sum(axis=(-2, -1))
</span></code></pre></div></div> <p>10. Create a function to produce a sliding window view of a 1D array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sliding_window</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Produce an array of sliding window views of arr
    Parameters:
    arr  - nd array
           Input array
    size - int, optional
           Size of sliding window
    </span><span class="sh">"""</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">stride_tricks</span><span class="p">.</span><span class="nf">as_strided</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span>
                                           <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span>
                                           <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">))</span>
</code></pre></div></div> <h2 id="future-resources">Future Resources</h2> <p>So far, we‚Äôve covered many of the basics of using NumPy for performing scientific computing. But there‚Äôs still a lot of material that you can learn from. To learn more about Numpy, I would definitely recommend the following:</p> <ul> <li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/index.html">Pandas Data Science Handbook</a> covers much more about Numpy. But it also has other libraries such as Pandas, Matplotlib very well explained with code walkthrough.</li> <li><a href="https://www.youtube.com/watch?v=cYugp9IN1-Q">Advanced NumPy - SciPy 2019</a> covers a lot of advanced material that we have not touched on in this post.</li> <li><a href="https://scipy-lectures.org/">Scipy Lecture Notes</a> is a good resource for learning libraries related to scientific computing such as NumPy, SciPy in Python</li> <li><a href="https://github.com/rougier/numpy-100">100 NumPy Exercises</a> is a good place to test your knowledge.</li> </ul> <div class="citations"> <d-cite key="harris_2020_array"> <d-cite key="jakevdp_2016_python_ds"> <d-cite key="alex_2022_numpy_intro"> <d-cite key="nunuz_2019_numpy_advanced"> <d-cite key="nicolas_p_rougier_2016_61020"> &lt;/div&gt; </d-cite></d-cite></d-cite></d-cite></d-cite></div>]]></content><author><name>Jay Gala</name></author><category term="python"/><category term="numpy"/><summary type="html"><![CDATA[This blog dives deeper in understanding the working of numpy along with exercises.]]></summary></entry></feed>