---
layout: page
title: talks
permalink: /talks/
description: ''
nav: true
nav_order: 2
---

##### **2024**

- An Empirical Study of In-context Learning in LLMs for Machine Translation  ([Slides](https://drive.google.com/file/d/1HCFpWcO6fUCwNnUfmtBXOSUCRaIaKrvx/view))
  <br> **SNLP Reading Group @ Microsoft Research India**

##### **2023**

- Developing SOTA MNMT Systems for Related Languages ([Slides](https://docs.google.com/presentation/d/1BW9N9Fi8X9QQYB_DmyjHm2w-0idKlfuydH3eswgpIqs/edit?usp=sharing))
  <br> **Tutorial at AACL-IJCNLP 2023**


- QLoRA: Efficient Finetuning of Quantized LLMs ([Paper](https://arxiv.org/abs/2305.14314))
  <br> **ML Collective NLP Reading Group**

- Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning ([Paper](https://arxiv.org/abs/2205.05638))
  <br> **ML Collective NLP Reading Group**

- Sparks of Artificial General Intelligence: Early experiments with GPT-4 ([Paper](https://arxiv.org/abs/2303.12712))
  <br> **ML Collective NLP Reading Group**

- Mixture of Soft Prompts for Controllable Data Generation ([Paper](https://arxiv.org/abs/2303.01580), [Annotated Paper](https://drive.google.com/file/d/1j8grlqnCABkdS2fa-VPni2RhhRzRAA8T/view))
  <br> **ML Collective NLP Reading Group**

- Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models ([Paper](https://arxiv.org/abs/2301.04213), [Slides](https://docs.google.com/presentation/d/1UJkXLiYS0FEtuKsgT_4HCT3WX2oFbuaFEeyeVVw7IjE/edit?usp=sharing))
  <br> **Cohere For AI NLP Reading Group**

- Attention with Linear Biases Enables Input Length Extrapolation ([Paper](https://arxiv.org/abs/2108.12409), [Annotated Paper](https://drive.google.com/file/d/1samZrqZjHMgirlbfGjffndbIbTPyJe5n/view?usp=sharing), [Video](https://www.youtube.com/watch?v=KvWpw5tZ1gc))
  <br> **ML Collective NLP Reading Group**

- Training a Language Model on a Single GPU in One Day ([Paper](https://arxiv.org/abs/2212.14034), [Annotated Paper](https://drive.google.com/file/d/1sbmaEkuUNi2m5x92XlXlw1A2pSbsnYd8/view?usp=sharing), [Video](https://www.youtube.com/watch?v=POUGSPZaMsk))
  <br> **ML Collective NLP Reading Group**

##### **2022**

- Scaling Instruction-Finetuned Language Models (Flan-PaLM) ([Paper](https://arxiv.org/abs/2210.11416), [Slides](https://docs.google.com/presentation/d/1CGHGUh27c--IcB2h1FSZDflZYow6Lqjohht2-AHnMhg/edit?usp=sharing))
  <br> **ML Collective NLP Reading Group**

- Knowledge Neurons in Pretrained Transformers Review ([Paper](https://arxiv.org/abs/2104.08696), [Slides](https://docs.google.com/presentation/d/1dFFwAlroEtOhyYY2sEJPTxVny8uN4KYWCHOkar2r7yg/edit?usp=sharing))
  <br> **ML Collective NLP Reading Group**

- Learning from Mistakes based on Class Weighting with Application to Neural Architecture Search ([Paper](https://arxiv.org/abs/2112.00275), [Slides](https://drive.google.com/file/d/1lasWeXPb6ABTzjo3a52_jHNI1crDeUK3/view?usp=sharing))
  <br> **PXie Lab**


##### **2021**

- Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge Review ([Paper](https://arxiv.org/pdf/2007.00849.pdf), [Slides](https://drive.google.com/file/d/1OKqP6_icQLwxY70u48M4U2Kf-J2r1aPp/view?usp=sharing))
  <br> **ML Collective NLP Reading Group**, **Unicode Research**

- Research Seminar for Undergraduates ([Slides](https://drive.google.com/file/d/1BNai1MVN7mx0wuAFgYVKfh48YF-D3PDS/view), [Video](https://www.youtube.com/watch?v=_0VpUNATCdY))
  <br> **Unicode Research**

- Deep Probabilistic Programming Review ([Paper](https://arxiv.org/pdf/1701.03757.pdf), [Slides](https://drive.google.com/file/d/1NBji25U3QGr-Zt6qBGTppVaPJ4BE_i1f/view), [Video](https://www.youtube.com/watch?v=nT8ISRrUixQ&list=PLob0yCmJjJ3U6vUrmExdTpMoRh43c1nXK&index=7))
  <br> **Unicode Research**

- Auto-encoding Variational Bayes Review ([Paper](https://arxiv.org/pdf/1312.6114.pdf), [Slides](https://drive.google.com/file/d/1or8TXSdZx93AIa5eqtAiyMKhKI5fnUiF/view), [Video](https://www.youtube.com/watch?v=bSQ129B_2jM&list=PLob0yCmJjJ3U6vUrmExdTpMoRh43c1nXK&index=6))
  <br> **Unicode Research**
