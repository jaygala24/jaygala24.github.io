<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Long Live Transformers! | Jay Gala</title> <meta name="author" content="Jay Gala"/> <meta name="description" content="This blog post explains the transformer and its building blocks as well as how they changed the field of NLP."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap" rel="stylesheet"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaygala24.github.io/blog/2021/long_live_transformers/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">@media(min-width:576px){.output img{display:block;margin-left:auto;margin-right:auto;width:60%}.small img{display:block;margin-left:auto;margin-right:auto;width:50%}}.citations{display:none}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Long Live Transformers!",
      "description": "This blog post explains the transformer and its building blocks as well as how they changed the field of NLP.",
      "published": "June 15, 2021",
      "authors": [
        {
          "author": "Jay Gala",
          "authorURL": "https://jaygala24.github.io",
          "affiliations": [
            {
              "name": "University of Mumbai",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%61%79%67%61%6C%61%32%34@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=lNn2qGoAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/1992915388" title="Semantic Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jaygala24" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jaygala24" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/jaygala24" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/cv.pdf">cv</a> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Long Live Transformers!</h1> <p>This blog post explains the transformer and its building blocks as well as how they changed the field of NLP.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#model-architecture">Model Architecture</a></div> <ul> <li><a href="#encoder-decoder">Encoder-Decoder</a></li> <li><a href="#self-attention">Self-Attention</a></li> <li><a href="#multi-head-attention">Multi-Head Attention</a></li> <li><a href="#positional-encoding">Positional Encoding</a></li> </ul> <div><a href="#follow-up-work">Follow-up Work</a></div> <ul> <li><a href="#transformer-xl">Transformer-XL</a></li> <li><a href="#reformer">Reformer</a></li> <li><a href="#linear-transformers">Linear Transformers</a></li> </ul> <div><a href="#references">References</a></div> </nav> </d-contents> <p>ðŸ“Œ Note: I would like to thank <a href="https://github.com/deep1401" target="_blank" rel="noopener noreferrer">Deep Gandhi</a> and <a href="https://github.com/PranjalChitale" target="_blank" rel="noopener noreferrer">Pranjal Chitale</a> for reviewing this blog and providing valuable feedback.</p> <p>This post assumes that you have a basic understanding of Neural Networks, specifically Recurrent Neural Networks (RNNs) and Bahdanauâ€™s attention mechanism. If you are new to the above-mentioned concepts or youâ€™d like to brush up, I would highly recommend reading <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener noreferrer">Understanding LSTM Networks</a> and Attentional Interfaces section from <a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener noreferrer">Attention and Augmented Recurrent Neural Networks</a>.</p> <p>Sequence-to-sequence (Seq2Seq) models have achieved a lot of success in various tasks such as machine translation, text summarization, question answering, etc. RNNs were the primary choice for seq2seq as they are useful for learning variable-length sequential data, but their sequential nature inherently inhibits parallelization. Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) widely dominated the RNN landscape. Although RNNs are good for capturing temporal dependencies, but they fail to preserve the complete context for long sequences. Attention mechanism overcame this drawback by providing us the ability to focus on different parts of encoded sequences during the decoding stage, thereby allowing the context to be preserved from beginning to end. Furthermore, attention between encoder and decoder has helped improve the performance of neural machine translation. Is there a way we can use attention for representations and parallelize all the computations without using RNNs?</p> <h2 id="introduction">Introduction</h2> <p>The Transformer is a deep learning language model that completely relies on attention mechanisms, specifically self-attention, to find relationships (global dependencies) between input and output. The Transformers have revolutionized the field of natural language processing and are the de facto standard for various language modeling tasks. The Transformers have been the backbone of state-of-the-art language models such as BERT, GPT, etc. Additionally, they are also being applied to computer vision and speech-related tasks. Now that we have got the idea of what the transformer is trying to achieve, so letâ€™s dive deeper into the basic building blocks of this transformer architecture.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/transformer_overview.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/transformer_overview.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/transformer_overview.gif-1400.webp"></source> <img src="/assets/img/long_live_transformers/transformer_overview.gif" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Visual overview of transformer (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h2 id="model-architecture">Model Architecture</h2> <p>The Transformer also follows the encoder-decoder architecture, same as the other neural transduction models. The encoder encodes an input sequence to a sequence of continuous representations, which the decoder uses to generate output sequence each time step during decoding. Thus, the decoding stage can be thought of as autoregressive language modeling, where the decoder finds the output sequence that is most probable conditioned on the input sequence. Mathematically, it can be formulated as follows:</p> \[P_{\theta}(y | x) = \prod_{t=1}^{T} P_{\theta}(y_t | y_{\lt t}, x)\] <p>where $x$ and $y$ denote the input and output sequence, $\theta$ denote the model parameters, and $t$ denote time step in the decoding stage.</p> <div class="output"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/transformer_architecture-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/transformer_architecture-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/transformer_architecture-1400.webp"></source> <img src="/assets/img/long_live_transformers/transformer_architecture.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Transformer Model Architecture (<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="encoder-decoder">Encoder-Decoder</h3> <p>The encoder consists of a stack of $N = 6$ identical layers, each containing two sub-layers, a multi-head self-attention layer, and a position-wise fully connected feed-forward network. Each sub-layer has a residual connection and layer normalization. Position-wise feed-forward network (FFN) consists of two linear transformations with ReLU activation in between. In FFN, the same linear transformation is applied across different positions. This can also be viewed as two convolutions with filter size 1.</p> \[FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\] <p>The decoder is quite similar to the encoder, except that the decoder contains two multi-head self-attention layers instead of a single layer in a stack of $N = 6$ identical layers. The first multi-head self-attention layer attends to decoder outputs generated so far and is masked in order to prevent positions from attending to future positions, whereas the second multi-head self-attention layer attends over the encoder stack output.</p> <p>The input and output sequences are embedded into a $d_{\text{model}}$ dimensional space, which is the usual step before feeding the sequence into the neural network. In addition, positional encoding is also applied to the embedded sequence, which gives a sense of order in the sequence. Weâ€™ll discuss positional encoding in detail later.</p> <h3 id="self-attention">Self-Attention</h3> <p>Attention mechanisms proposed by <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener noreferrer">Bahdanau et al., 2014</a> and <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener noreferrer">Luong et al., 2015</a> have been ubiquitously used to improve performance in various NLP tasks. As described previously that it is a mechanism that allows the neural network to make predictions by selectively focusing on a given sequence of data.</p> <div class="note"> <em> Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation. (Excerpt from a <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener noreferrer">blog post</a> by Lilian Weng). </em> </div> <p><br></p> <p>Self-attention can be considered as a permutation invariant sequence-to-sequence operation where we map a query and a set of key-value pairs to an output. Here, the input and output in this self-attention operation are vectors. Thus, self-attention can be viewed as similar to the gating mechanism in LSTMs or GRUs, which decides how much information to store using different gates.</p> <div class="small"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/scaled_dot_prod_attn-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/scaled_dot_prod_attn-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/scaled_dot_prod_attn-1400.webp"></source> <img src="/assets/img/long_live_transformers/scaled_dot_prod_attn.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Scaled Dot-Product Attention (<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">image source</a>) </div> <p>The Transformer relies on <code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> as a self-attention mechanism. Given a query matrix $Q$, a key matrix $K$, and a value matrix $V$, the output is a weighted sum of the values where the weight assigned to each value is determined by a dot product (compatibility function) of the query with the corresponding key. Mathematically, it can be expressed as follows:</p> \[\text{Attention}(Q, K, V) = \text{softmax}(\frac{Q K^T}{\sqrt{d_k}}) V \\ \text{where}\ Q \in \mathbb{R}^{\mathrm{|Q|\times d_k}}, K \in \mathbb{R}^{\mathrm{|K|\times d_k}}, V \in \mathbb{R}^{\mathrm{|V|\times d_v}}\] <p>The scaled dot-product attention faster and efficient since it is simply matrix multiplication. $\sqrt{d_k}$ is just a scaling or temperature factor which is used to normalize the dot-product in order to avoid uneven gradient flows. As we discussed above that, we perform the scaled dot-product attention on matrices due to efficient computations.</p> <p>If we were to formulate the same expression using the query $q_i$, key $k_j$ , and value $v_j$ vectors, then it would be as follows:</p> \[\text{Attention}(q_i, k_j, v_j) = \text{softmax}(\frac{q_i k_j^T}{\sqrt{d_k}}) v_j\] <p>Visually, the scaled-dot product attention can be expressed as follows:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/self_attn_calc_matrix_form-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/self_attn_calc_matrix_form-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/self_attn_calc_matrix_form-1400.webp"></source> <img src="/assets/img/long_live_transformers/self_attn_calc_matrix_form.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Visual representation of self-attention calculation in matrix form (<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>Although it may seem that the single self-attention block is sufficient to capture all contextual relevance for a particular input word in the sequence, but in practice, the word may have multiple senses, which makes capturing complete context difficult. In order to solve the above issue, the authors introduced a multi-head attention mechanism which expands the modelâ€™s ability to focus on different positions and allows to encode multiple relationships and nuances for a particular word in the sequence. In short, the multi-head attention mechanism is nothing but repeating scaled dot-product attention $h = 8$ times (i.e., over each subspace) in parallel.</p> <div class="output"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/multi_head_attn-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/multi_head_attn-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/multi_head_attn-1400.webp"></source> <img src="/assets/img/long_live_transformers/multi_head_attn.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Multi-Head Attention (<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">image source</a>) </div> <div class="note"> <em> According to the authors, multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. </em> </div> <p><br></p> <p>In the multi-head attention layer, the inputs query matrix $Q$, key matrix $K$, and a value matrix $V$ are first linearly transformed using weight matrices $W_i^Q$, $W_i^K$, $W_i^V$ for each attention head $i$. Then, the attention head outputs computed in parallel are concatenated and linearly transformed using weight matrix $W^O$.</p> <p>Mathematically, it can be expressed as follows:</p> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O\] \[\text{where}\ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\] \[W_i^Q \in \mathbb{R}^{\mathrm{d_{model}\times d_k}}, W_i^K \in \mathbb{R}^{\mathrm{d_{model}\times d_k}}, W_i^V \in \mathbb{R}^{\mathrm{ d_{model}\times d_v}}, W_i^O \in \mathbb{R}^{\mathrm{hd_v\times d_{model}}}\] <p>Visually, the multi-head attention mechanism can be expressed as follows:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/multi_head_calc_matrix_form-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/multi_head_calc_matrix_form-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/multi_head_calc_matrix_form-1400.webp"></source> <img src="/assets/img/long_live_transformers/multi_head_calc_matrix_form.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Visual representation of multi-head calculation in matrix form (<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="positional-encoding">Positional Encoding</h3> <p>As discussed above, the self-attention is permutation invariant, and we have ditched RNNs that inherently order the sequence during processing. Therefore, we need some way to incorporate position and order of sequence. In order to achieve the above, positional encoding was introduced, which adds positional information to each word in the sequence giving a sense of order.</p> <p>A simple trivial solution would be to add index position to the word embeddings. However, there is a catch in the current solution that the word embedding values could get quite large for longer sequences and destroy the actual information in the embedding. Additionally, our model may not observe samples with specific lengths during training and result in poor generalization.</p> <p>The authors proposed a sinusoidal positional encoding which is defined as follows:</p> \[\begin{aligned} \text{PE}_{(pos, 2i)} &amp;= \text{sin}(\frac{pos}{10000^{2i/d_{model}}}) \\ \text{PE}_{(pos, 2i + 1)} &amp;= \text{cos}(\frac{pos}{10000^{2i/d_{model}}}) \end{aligned}\] <p>where $pos$ is the position and $i$ is the dimension.</p> <p>Each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions ranging from $2\pi$ to $10000 \cdot 2\pi$.</p> <p>Visually, the positional encoding can be expressed as follows:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/positional_encoding-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/positional_encoding-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/positional_encoding-1400.webp"></source> <img src="/assets/img/long_live_transformers/positional_encoding.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Visual representation of positional encoding (<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener noreferrer">image source</a>) </div> <p>In the above diagram, we can observe that we may get the same position embedding values across different time steps for a particular dimension. For example, consider the curve dim 5 across time steps 20, 60, and 100. But if we consider curves from all the dimensions, we will end up getting different position embedding values across time steps. <em>(Please check <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" target="_blank" rel="noopener noreferrer">this post</a> to learn more about positional encoding in the transformer.)</em></p> <p>You must be wondering that the positional encoding information will be lost as we process the embedded sequence in higher layers. How do we ensure that the positional information is preserved? Remember, we had used residual connections in each encoder and decoder layer. These residual connections help us carry positional information to higher layers.</p> <p>Yay! we made it to the end. So letâ€™s highlight the motivation/benefits of the self-attention mechanism based on whatever we have discussed so far.</p> <ol> <li>Computational complexity per layer is reduced due to the removal of recurrent layers</li> <li>It is trivial to parallelize the amount of computations per layer</li> <li>It provides us gating interactions observed similar to LSTMs and GRUs</li> <li>Constant path lengths between any two positions in the sequence</li> <li>It also helps in yielding more interpretable models due to multiple attention heads</li> </ol> <h2 id="follow-up-work">Follow-up Work</h2> <p>The Transformer proposed by Vaswani et al., 2017 was the first of its kind developed to process sequential data without using RNNs. Over the years, several state-of-the-art models have been derived based on the transformer architecture, and networks have become larger and larger due to the computational efficiency achieved compared to previously used RNNs. They are also now being applied to computer vision tasks described by <a href="https://arxiv.org/pdf/2012.12556.pdf" target="_blank" rel="noopener noreferrer">Han et al., 2021</a>.</p> <h3 id="transformer-xl">Transformer-XL</h3> <p>Although the transformer revolutionized natural language processing due to the self-attention mechanism, they have some limitations, such as fixed-length context and limited attention span. This means that model can only attend to the elements of a fixed-length segment leading to context fragmentation. Furthermore, this prevents the model from capturing long-term dependencies and allows no flow of the information across different segments.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/vanilla_transformer_training.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/vanilla_transformer_training.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/vanilla_transformer_training.gif-1400.webp"></source> <img src="/assets/img/long_live_transformers/vanilla_transformer_training.gif" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Vanilla Transformer with a fixed context length at training time (<a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" target="_blank" rel="noopener noreferrer">image source</a>) </div> <p>To overcome this issue, <a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener noreferrer">Dai et al., 2019</a> proposed Transformer-XL, which allows the flow of information by reusing hidden states between segments and enabling the ability to learn long-term dependencies.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/transformer_xl_training.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/transformer_xl_training.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/transformer_xl_training.gif-1400.webp"></source> <img src="/assets/img/long_live_transformers/transformer_xl_training.gif" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Transformer-XL with segment-level recurrence at training time (<a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="reformer">Reformer</h3> <p>The vanilla Transformer is very slow for processing long sequences since the self-attention takes $O(n^2)$ memory and computation where $n$ is the sequence length. Therefore, training these models is costly due to high memory and computation requirements. To overcome this limitation, <a href="https://arxiv.org/pdf/2001.04451.pdf" target="_blank" rel="noopener noreferrer">Kitaev et al., 2020</a> proposed Reformer, a Transformer model designed to process long sequences efficiently without much memory and computation resource.</p> <p><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank" rel="noopener noreferrer">Locality Sensitive Hashing (LSH)</a> is employed to reduce the complexity of attending over long sequences. The idea behind LSH is that similar items will end up in the same buckets with high probability. We use this idea for bucketing similar vectors instead of scanning over all the pairs of vectors. The vectors in the same bucket will only attend to each other during the self-attention computation. Additionally, they also use reversible residual layers instead of standard residuals, allowing more efficient use of memory since the activations are stored only once instead of $N$ times, where $N$ is the number of layers. This reduces the complexity from $O(n^2)$ to $O(n\ \text{log}\ n)$</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/long_live_transformers/lsh_attention-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/long_live_transformers/lsh_attention-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/long_live_transformers/lsh_attention-1400.webp"></source> <img src="/assets/img/long_live_transformers/lsh_attention.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Visual overview of LSH Attention (<a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="linear-transformers">Linear Transformers</h3> <p>This work also focuses on reducing the complexity of dot-product attention and draws a parallel between Transformers and RNNs. <a href="https://arxiv.org/pdf/2006.16236.pdf" target="_blank" rel="noopener noreferrer">Katharopoulos et al., 2020</a> formulates the self-attention mechanism as a linear dot-product of kernel maps as follows:</p> \[\text{Attention}(Q, K, V) = \frac{\sum_{j=1}^N \text{sim}(Q_i, K_j) V_j}{\sum_{j=1}^N \text{sim}(Q_i, K_j)}\] <p>The above formulation is equivalent to the vanilla Transformer if we use the exponential dot-product as the similarity function.</p> \[\text{sim}(Q, K) = \text{exp}(\frac{Q^T K}{\sqrt{d_k}})\] <p>The authors propose to use kernel maps $\phi(x)$ to aggregate the information between all the elements in the sequence, thereby allowing us to compute the inner product between infinite-dimensional spaces efficiently. This new formulation leads to better memory and computation efficiency reducing the complexity from $O(n^2)$ to $O(n)$.</p> \[V_i^\prime = \frac{\sum_{j=1}^N \phi(Q_i)^T \phi(K_j) V_j}{\sum_{j=1}^N \phi(Q_i)^T \phi(K_j)} = \frac{\phi(Q_i)^T \sum_{j=1}^N \phi(K_j) V_j}{\phi(Q_i)^T \sum_{j=1}^N \phi(K_j)}\] <p>The above equation in the vectorized form is as follows:</p> \[(\phi(Q)\ \phi(K)^T)\ V = \phi(Q)\ (\phi(K)^T\ V)\] <p>Although the vanilla Transformers perform better than the linear Transformers, but there is a significant improvement in speed for linear Transformers. The authors have also provided a demo which you can play with at this <a href="https://linear-transformers.com/" target="_blank" rel="noopener noreferrer">link</a>.</p> <p>There are many follow-up works based on the vanilla Transformer architecture. Unfortunately, it is not possible to highlight every piece of work in this blog. Instead, I have tried to give intuition behind the few architectures above. I would definitely advise you to check the following <a href="https://paperswithcode.com/methods/category/transformers" target="_blank" rel="noopener noreferrer">webpage</a> to explore other transformer architectures as well as read the survey paper by <a href="https://arxiv.org/pdf/2106.04554.pdf" target="_blank" rel="noopener noreferrer">Lin et al., 2021</a>.</p> <p>If you would like to play around with the vanilla Transformer, here is the <a href="https://colab.research.google.com/github/jaygala24/pytorch-implementations/blob/master/Attention%20Is%20All%20You%20Need.ipynb" target="_blank" rel="noopener noreferrer">colab notebook</a> created by me for English to German translation.</p> <div class="citations"> <d-cite key="Vaswani2017AttentionIA"> <d-cite key="Kaiser2017AttentionNN"> <d-cite key="Google2017Transformer"> <d-cite key="Alammar2018Transformer"> <d-cite key="Harvard2018Transformer"> <d-cite key="weng2018attention"> <d-cite key="positional_2019_stack_exchange"> <d-cite key="positional_2019_kazemnejad"> <d-cite key="Dai2019TransformerXLAL"> <d-cite key="Google2019TransformerXL"> <d-cite key="Kitaev2020ReformerTE"> <d-cite key="Google2020Reformer"> <d-cite key="Katharopoulos2020TransformersAR"> &lt;/div&gt; </d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 Jay Gala. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: October 23, 2025. </div> </footer> <d-bibliography src="/assets/bibliography/2021-06-15-long_live_transformers.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-BD3M6B4H2R"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-BD3M6B4H2R");</script> </body> </html>