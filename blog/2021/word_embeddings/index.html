<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Word Embeddings | Jay Gala</title> <meta name="author" content="Jay Gala"/> <meta name="description" content="This blog goes through the two popular techniques used for embeddings i.e. Word2Vec and GloVe in detail understanding the math along with the code implementation in PyTorch."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap" rel="stylesheet"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaygala24.github.io/blog/2021/word_embeddings/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">@media(min-width:576px){.output-plot img{display:block;margin-left:auto;margin-right:auto;width:50%}}.citations{display:none}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Word Embeddings",
      "description": "This blog goes through the two popular techniques used for embeddings i.e. Word2Vec and GloVe in detail understanding the math along with the code implementation in PyTorch.",
      "published": "April 20, 2021",
      "authors": [
        {
          "author": "Jay Gala",
          "authorURL": "https://jaygala24.github.io",
          "affiliations": [
            {
              "name": "University of Mumbai",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%61%79%67%61%6C%61%32%34@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=lNn2qGoAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/1992915388" title="Semantic Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jaygala24" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jaygala24" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/jaygala24" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/cv.pdf">cv</a> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Word Embeddings</h1> <p>This blog goes through the two popular techniques used for embeddings i.e. Word2Vec and GloVe in detail understanding the math along with the code implementation in PyTorch.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#word2vec">Word2Vec</a></div> <ul> <li><a href="#overview">Overview</a></li> <li><a href="#skip-gram-model">Skip-Gram Model</a></li> <li><a href="#negative-sampling">Negative Sampling</a></li> <li><a href="#subsampling-frequent-words">Subsampling Frequent Words</a></li> <li><a href="#skip-gram-implementation">Skip-Gram Implementation</a></li> </ul> <div><a href="#glove">GloVe</a></div> <ul> <li><a href="#overview">Overview</a></li> <li><a href="#co-occurrence-matrix">Co-occurrence matrix</a></li> <li><a href="#mathematics">Mathematics</a></li> <li><a href="#glove-implementation">GloVe Implementation</a></li> </ul> <div><a href="#references">References</a></div> </nav> </d-contents> <p>Humans use language as a way of communication for exchanging ideas and knowledge with others. Words are an integral part of the language that represents the <a href="https://en.wikipedia.org/wiki/Denotational_semantics" target="_blank" rel="noopener noreferrer">denotational semantics</a>, i.e., the meaning of a word. Humans are good at processing and understanding the idea that the words convey. Hence, we can share information about an image or an incident using a short string, assuming that we have some previous context. For example, a single word, “traffic,” conveys the information equivalent to a picture representing several vehicles stuck in a jam as shown in the figure below. However, computers are not good at understanding the ideas from the words, and we need to way to encode these ideas which computers can understand, i.e., in the form of numbers. These encoded representation are helpful for solving complex NLP tasks and are known as word embeddings (word vectors of a certain dimensions representing the idea of the word). This encoding is known as word embeddings (word vectors) which can help us use these representations for solving the complex NLP tasks.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/traffic_jam_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/traffic_jam_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/traffic_jam_pic-1400.webp"></source> <img src="/assets/img/word_embeddings/traffic_jam_pic.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Picture of a traffic jam (<a href="https://bit.ly/3aq0qL9" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h2 id="word2vec">Word2Vec</h2> <h3 id="overview">Overview</h3> <p>Previously, neural language models involved the first stage as learning a distributed representation for words and using these representations in the later stages for obtaining prediction. The main idea of the word2vec is also based on these neural language models where we use the hidden layer of a neural network to learn continuous representations of words which we call embeddings. Here, we discard the output layer of a trained neural network. Word2Vec model presents two algorithms:</p> <ul> <li>Continuous Bag of Words (CBOW): Predict center word based on the context words.</li> <li>Skip Gram: Predict the context words based on the center word.</li> </ul> <p>The figure below illustrates the two algorithms:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/word2vec_algorithms-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/word2vec_algorithms-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/word2vec_algorithms-1400.webp"></source> <img src="/assets/img/word_embeddings/word2vec_algorithms.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Word2Vec Model Architectures (<a href="https://arxiv.org/abs/1301.3781" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="skip-gram-model">Skip-Gram Model</h3> <p>From now onwards, we will look into the skip-gram model for word embeddings. The task is formulated as predicting the context words within a fixed window of size m given the center word. The visual illustration of above idea is shown below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/skip_gram_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/skip_gram_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/skip_gram_overview-1400.webp"></source> <img src="/assets/img/word_embeddings/skip_gram_overview.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Skip-Gram Model Overview (<a href="https://stanford.io/32wNQWe" target="_blank" rel="noopener noreferrer">image source</a>) </div> <p>Word pairs from the large corpus of text for a fixed window size $m$ are used for training the neural network. These word pairs are formed by looking at a fixed window size $m$ before and after the center word. This window size is a hyperparameter that you can play around with, but the authors found that window size 5 seems to work well in general. Having a smaller window size means that you are capturing minimal context. On the other hand, if your window size is too large, you are capturing too much context, which might not help you obtain specific meanings. For the above example with window size 2, the training pairs would be the following.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    [(into, problems), (into, turning), (into, banking), (into, crises)]
</code></pre></div></div> <p>Since every word is represented by a vector, so the objective is to iteratively maximize the probability of context words $o$ given the center word $c$ for each position $t$ and adjust the word vectors.</p> \[L(\theta) = \prod_{t=1}^{T}\prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j}\ |\ w_t\ ;\ \theta)\] <p>where $L$ is the likelihood estimation and $\theta$ is the vector representation to be optimized.</p> <p>In order to avoid the floating-point overflow and simple gradient calculation, we take the apply logarithm to the above likelihood estimation. The cost function is given as follows:</p> \[J(\theta) = - \frac{1}{T}\ log\ L(\theta) = - \frac{1}{T}\ \prod_{t=1}^{T}\prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j}\ |\ w_t\ ;\ \theta)\] <p>Here we are minimizing the cost function, which means that we are maximizing the likelihood estimation, i.e., predictive accuracy.</p> <p>Likelihood estimation for a context word $o$ given the center word $c$ is as follows:</p> \[P(o\ |\ c) = \frac{exp(u_o^T v_c)}{\sum_{w \in V}\ exp(u_w^T v_c)}\] <p>where</p> <ul> <li>$v_w$ and $u_w$ are the center word and context word vector representations</li> <li>$u_o^T v_c$ represents the dot product which is used as a similarity measure between context word $o$ and center word $c$</li> <li>$V$ represents the vocabulary</li> </ul> <p>In order to express this similarity measure in terms of probability, we normalize over the entire vocabulary (the idea of using softmax) and $exp$ is used to quantify the dot product to a positive value.</p> <p>Computing the normalizing factor for every word is too much expensive, which is why the authors came up with some tricks which reduce the computational cost and speed up the training.</p> <h3 id="negative-sampling">Negative Sampling</h3> <p>The main idea of the negative sampling is to differentiate data from noise, i.e., train a binary logistic regression for classifying a true pair (center word and context word) against several noise pairs (center word and random word). So now our problem is reduced to $K + 1$ labels classification instead of $V$ words ($K \ll V$), which means that weights will only be updated for $K + 1$ words whereas weights for all the words were updated. In general, we choose 5 negative words other than the context window around the center word ($K = 5$). We want the context words to have a higher probability than the sampled negative words.</p> <p>The new objective function (cost function) is given as follows:</p> \[J_{neg-sample}(\theta) = -\ log\ (\sigma(u_o^T v_c)) - \sum_{k=1}^{K}\ log\ (\sigma(-u_k^T v_c))\] <p>where</p> <ul> <li>$\sigma$ represents sigmoid</li> <li>first term represents the estimation for true pair</li> <li>second term represents the estimation for negative samples</li> </ul> <p>The authors found that the unigram distribution $U(w)^{3/4}$ works well than the other unigram and uniform distribution choices for sampling noise. The intuition is that raised to $3/4$ factor brings down the probability for more frequent words.</p> \[P_n(w) = \frac{U(w)^{3/4}}{Z}\] <p>where $Z$ is the normalization term.</p> <h3 id="subsampling-frequent-words">Subsampling Frequent Words</h3> <p>Word2vec has been trained on a very large corpus of text in which frequently occurring words do not contribute significantly to the meaning of a word. Common function words such as “the”, “as”, “a” provide structure to the sentence but don’t help in learning good quality word representation as they occur in context with many words in the corpus. For example, the co-occurrence of “New”, “York” benefits the model in capturing better meaningful representation than the co-occurrence of “New”, “the”. The authors introduce a subsampling technique that discards the high-frequency words based on the probability formula computed for each word $w_i$ which is given below:</p> \[P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}\] <p>where $t$ is a chosen threshold, typically around $10^{-5}$.</p> <h3 id="skip-gram-implementation">Skip-Gram Implementation</h3> <p>Here we will be using text corpus of cleaned wikipedia articles provided by Matt Mahoney.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">s3</span><span class="p">.</span><span class="n">amazonaws</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">video</span><span class="p">.</span><span class="n">udacity</span><span class="o">-</span><span class="n">data</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">topher</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="n">October</span><span class="o">/</span><span class="mi">5</span><span class="n">bbe6499_text8</span><span class="o">/</span><span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">"</span><span class="s">retina</span><span class="sh">"</span>

<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check if gpu is available since training is faster
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Word2VecDataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Prepares the training data for the word2vec neural network.
            Params:
                corpus (string): corpus of words
                min_count (int): words with minimum occurrence to consider
                window_size (int): context window size for generating word pairs
                threshold (float): threshold used for subsampling words
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
        <span class="n">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="c1"># only consider the words that occur atleast 5 times in the corpus 
</span>        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">({</span><span class="n">word</span><span class="p">:</span><span class="n">count</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">})</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="nf">most_common</span><span class="p">())}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

        <span class="c1"># create prob dist based on word frequency
</span>        <span class="n">word_freq</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">unigram_dist</span> <span class="o">=</span> <span class="n">word_freq</span> <span class="o">/</span> <span class="n">word_freq</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

        <span class="c1"># create prob dist for negative sampling
</span>        <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">unigram_dist</span> <span class="o">**</span> <span class="mf">0.75</span>
        <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

        <span class="c1"># get prob for drop words
</span>        <span class="n">self</span><span class="p">.</span><span class="n">word_drop_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">threshold</span> <span class="o">/</span> <span class="n">word_freq</span><span class="p">)</span>

        <span class="c1"># create the training corpus subsampling frequent words
</span>        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> 
                          <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span> <span class="ow">and</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">word_drop_prob</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]]]</span>

        <span class="c1"># create word pairs for corpus
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">generate_word_pairs</span><span class="p">()</span>
    

    <span class="k">def</span> <span class="nf">generate_word_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the pairs of center and context words based on the context window size.
        </span><span class="sh">"""</span>
        <span class="n">word_pair_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">current_idx</span><span class="p">,</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="c1"># find the start and end of context window
</span>            <span class="n">left_boundary</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">right_boundary</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">))</span>

            <span class="c1"># obtain the context words and center words based on context window
</span>            <span class="n">context_word_ids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">left_boundary</span><span class="p">:</span><span class="n">current_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">right_boundary</span><span class="p">]</span>
            <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span><span class="p">]</span>
            
            <span class="c1"># add the word pair to the training set
</span>            <span class="k">for</span> <span class="n">context_word_id</span> <span class="ow">in</span> <span class="n">context_word_ids</span><span class="p">:</span>
                <span class="n">word_pair_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">center_word_id</span><span class="p">,</span> <span class="n">context_word_id</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">word_pair_ids</span> <span class="o">=</span> <span class="n">word_pair_ids</span>


    <span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the batches for training the network.
            Params:
                batch_size (int): size of the batch
            Returns:
                batch (torch tensor of shape (batch_size, 2)): tensor of word pair ids for a given batch
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">word_pair_ids</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">word_pair_ids</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">get_negative_samples</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Samples negative word ids for a given batch.
            Params:
                batch_size (int): size of the batch
                n_samples (int): number of negative samples
            Returns:
                neg_samples (torch tensor of shape (batch_size, n_samples)): tensor of negative sample word ids
                    for a given batch
        </span><span class="sh">"""</span>
        <span class="n">neg_samples_ids</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">),</span> 
                                       <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">noise_dist</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">neg_samples_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read the file and initialize the Word2VecDataset
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">text8</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nc">Word2VecDataset</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SkipGramModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Skip Gram variant of Word2Vec with negative sampling for learning word 
            embeddings. Uses the concept of predicting context words given the 
            center word.
            Params:
                vocab_size (int): number of words in the vocabulary
                embed_dim (int): embeddings of dimension to be generated
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SkipGramModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="c1"># embedding layers for input (center) and output (context) words
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1"># initialize the embeddings with uniform dist
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_ids</span><span class="p">,</span> <span class="n">pos_out_ids</span><span class="p">,</span> <span class="n">neg_out_ids</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Trains the Skip Gram variant model and updates the weights based on the
            criterion.
            Params:
                in_ids (torch tensor of shape (batch_size,)): indexes of the input words for a batch
                pos_out_ids (torch tensor of shape (batch_size,)): indexes of the output words (true pairs) for a batch
                neg_out_ids (torch tensor of shape (batch_size, number of negative samples)): 
                    indexes of the noise words (negative pairs) for a batch
        </span><span class="sh">"""</span>
        <span class="n">emb_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_in</span><span class="p">(</span><span class="n">in_ids</span><span class="p">)</span>
        <span class="n">pos_emb_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_out</span><span class="p">(</span><span class="n">pos_out_ids</span><span class="p">)</span>
        <span class="n">neg_emb_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_out</span><span class="p">(</span><span class="n">neg_out_ids</span><span class="p">)</span>

        <span class="c1"># calculate loss for true pair
</span>        <span class="c1"># ----------------------------
</span>        <span class="c1"># step 1 is calculate the dot product between the input and output word embeddings
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">pos_emb_out</span><span class="p">,</span> <span class="n">emb_in</span><span class="p">)</span>      <span class="c1"># element-wise multiplication
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">pos_loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>           <span class="c1"># sum the element-wise components
</span>        
        <span class="c1"># step 2 is to calculate the log sogmoid of dot product
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">logsigmoid</span><span class="p">(</span><span class="n">pos_loss</span><span class="p">)</span>

        <span class="c1"># calculate loss for negative pairs
</span>        <span class="c1"># ----------------------------------
</span>        <span class="c1"># step 1 is calculate the dot product between the input and output word embeddings
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="o">-</span><span class="n">neg_emb_out</span><span class="p">,</span> <span class="n">emb_in</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">()</span>   <span class="c1"># matrix-matrix multiplication
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">neg_loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                               <span class="c1"># sum the element-wise components
</span>
        <span class="c1"># step 2 is to calculate the log sogmoid of dot product
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">logsigmoid</span><span class="p">(</span><span class="n">neg_loss</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">pos_loss</span> <span class="o">+</span> <span class="n">neg_loss</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># intialize the model and optimizer
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">SkipGramModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training the network
</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_neg_samples</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Start of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">get_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># get the negative samples
</span>        <span class="n">noise_word_ids</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">get_negative_samples</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">n_neg_samples</span><span class="p">)</span>

        <span class="c1"># load tensor to GPU
</span>        <span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target_word_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">noise_word_ids</span> <span class="o">=</span> <span class="n">noise_word_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># forward pass
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">,</span> <span class="n">target_word_ids</span><span class="p">,</span> <span class="n">noise_word_ids</span><span class="p">)</span>

        <span class="c1"># backward pass, optimize
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epochs: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="se">\t</span><span class="s">Avg training loss: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="se">\t</span><span class="s">Ellapsed time: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> s</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">End of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the trained embeddings from the model
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># number of words to be visualized
</span><span class="n">viz_words</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># projecting the embedding dimension from 300 to 2
</span><span class="n">tsne</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">()</span>
<span class="n">embed_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[:</span><span class="n">viz_words</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># plot the projected embeddings
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">viz_words</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">idx2word</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="p">(</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div></div> <h2 id="glove">GloVe</h2> <h3 id="overview-1">Overview</h3> <p>Previously, there were two main directions for learning distributed word representations: 1) count-based methods such as Latent Semantic Analysis (LSA) 2) direct prediction-based methods such as Word2Vec. Count-based methods make efficient use of statistical information about the corpus, but they do not capture the meaning of the words like word2vec and perform poorly on analogy tasks such as <code class="language-plaintext highlighter-rouge">king - queen = man - woman</code>. On the other hand, direct prediction-based methods capture the meaning of the word semantically and syntactically using local context but fail to consider the global count statistics. This is where GloVe comes into the picture and overcomes the drawbacks of both approaches by combining them. The author proposed a global log bilinear regression model to learn embeddings based on the co-occurrence of words. Note that the GloVe does not use a neural network for learning word vectors.</p> <h3 id="co-occurrence-matrix">Co-occurrence matrix</h3> <p>The authors used a co-occurrence matrix with a context window of fixed size $m$ to learn the word embeddings. Let’s try to generate this matrix for the below toy example with a context window of size 2:</p> <ul> <li>I like deep learning</li> <li>I like NLP</li> <li>I enjoy flying</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/co_occurrence_matrix-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/co_occurrence_matrix-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/co_occurrence_matrix-1400.webp"></source> <img src="/assets/img/word_embeddings/co_occurrence_matrix.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Co-occurrence Matrix Example (<a href="https://stanford.io/3n4FH4H" target="_blank" rel="noopener noreferrer">image source</a>) </div> <h3 id="mathematics">Mathematics</h3> <p>Before we move ahead, let’s get familiarized with some notations.</p> <ul> <li>$X$ denotes the word-word co-occurrence matrix</li> <li>$X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$</li> <li>$X_i$ = $\sum_{k}{X_{ik}}$ denotes the number of times any word $k$ appearing in context of word $i$ and $k$ represents the total number of distinct words that appear in context of word $i$)</li> <li>$P_{ij} = P(j | i) = \frac{X_{ij}}{X_i}$ denotes the co-occurence probablity i.e. probability that word $j$ appears in the context of word $i$</li> </ul> <p>The denominator term in the co-occurrence probability accounts for global statistics, which word2vec does not uses. The main idea behind the GloVe is to encode meaning using the ratios of co-occurrence probabilities. Let’s understand the above by deriving the linear meaning components for the following words based on co-occurrence probability.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/word_embeddings/co_occurrence_probs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/word_embeddings/co_occurrence_probs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/word_embeddings/co_occurrence_probs-1400.webp"></source> <img src="/assets/img/word_embeddings/co_occurrence_probs.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Co-occurrence Probabilities Example (<a href="http://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener noreferrer">image source</a>) </div> <p>The matrix shows the co-occurrence probabilities for the words from the concept of the thermodynamic phases of water (i.e., “ice” and “steam”). The first two rows represent the co-occurrence probabilities for the words $ice$ and “steam”, whereas the last row represents their ratios. We can observe the following:</p> <ul> <li>ratio is not neural for closely related words such as “solid” and “ice” or “gas” and “steam”</li> <li>ratio is neutral for words relevant to “ice” and “steam” both or not completely irrelevant to both</li> </ul> <p>The ratio of co-occurrence proababilities is a good starting point for learning word embeddings. Let’s start with the most general function $F$ parametrized by 3 word vectors ($w_i$, $w_j$ and $\tilde{w_k}$) given below.</p> \[F(w_i, w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}\] <p>where $w, \tilde{w} \in \mathrm{R^d}$ and $\tilde{w}$ represent the separate context words.</p> <p>How do we choose $F$?</p> <p>There can be many possibilities for choosing $F$ but imposing some constraints allows us to restrict $F$ and select a unique choice. The goal is to learn word vectors (embeddings) that can be projected in the word vector space. These vector spaces are inherently linear, i.e., think of vectors as a line in $\mathrm{R^d}$ space, so the most intuitive way is to take vector differences which makes our function $F$ as follows:</p> \[F(w_i - w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}\] <p>We see that the right-hand side of the above equation is a scalar. Choosing a complex function such as a neural network would introduce non-linearities since our primary goal is to capture the linear meaning components from word vector space. Here, we take dot product on the left-hand side to make it a scalar similar to the right-hand side.</p> \[F((w_i - w_j)^T \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}\] <p>We also need to preserve symmetry for the distinction between a word and a context word which means that if $ice$ can be used as a context word for $water$, then $water$ can also be used as a context word for $ice$. In a simple, it can be expressed as $w \leftrightarrow \tilde{w}$. This is also evident from our co-occurrence matrix since $X \leftrightarrow X^T$. In order to restore the symmetry, we require that function $F$ is a homomorphism between groups $(\mathrm{R, +})$ and $(\mathrm{R, \times})$.</p> <div class="note"> <em> Given two groups, $\small (G, ∗)$ and $\small (H, \cdot)$, a group homomorphism from $\small (G, ∗)$ to $\small (H, \cdot)$ is a function $\small h : G \rightarrow H$ such that for all $u$ and $v$ in $\small G$ it holds that $\small h(u * v) = h(u) \cdot h(v)$. </em> </div> \[\begin{align*} F((w_i - w_j)^T \tilde{w_k}) &amp;= F(w_i^T \tilde{w_k} + (-w_j^T \tilde{w_k})) \\ &amp;= F(w_i^T \tilde{w_k}) \times F(-w_j^T \tilde{w_k}) \\ &amp;= F(w_i^T \tilde{w_k}) \times F(w_j^T \tilde{w_k})^{-1} \\ &amp;= \frac{F(w_i^T \tilde{w_k})}{F(w_j^T \tilde{w_k})} \end{align*}\] <p>So if we recall the $F$ in terms of co-occurrence probabilities, we get the following.</p> \[F(w_i^T \tilde{w_k}) = P_{ik} = \frac{X_{ik}}{X_i}\] <p>Since we are expressing $F$ in terms of probability which is a non-negative term, so we apply exponential to dot product $w_i^T \tilde{w_k}$ and then take logarithm on both sides.</p> \[w_i^T \tilde{w_k} = log(P_{ik}) = log(X_{ik}) - log(X_i)\] <p>On the right hand, the term $log(X_i)$ is independent of $k$ so it can be absorbed into a bias $b_i$ for $w_i$. Finally, we add bias $\tilde{b_k}$ for $\tilde{w_k}$ to restore the symmetry.</p> \[w_i^T \tilde{w_k} + b_i + \tilde{b_k} = log(X_{ik})\] <p>The above equation leads to our objective function, a weighted least squares regression model where we use the weighting function $f(X_{ij})$ for word-word co-occurrences.</p> \[J = \sum_{i,j = 1}^{V}f(X_{ij}) (w_i^T \tilde{w_k} + b_i + \tilde{b_k} - logX_{ik})^2\] <p>where $V$ is the size of the vocabulary.</p> <p>Here, the weighting function is defined as follows:</p> \[f(x) = \begin{cases} (x / x_{max})^{\alpha} &amp; \text{if}\ x &lt; x_{max} \\ 1 &amp; \text{otherwise} \end{cases}\] <p>where $x_{max}$ is the cutoff of the weighting function and $\alpha$ is power scaling similar to Word2Vec.</p> <h3 id="glove-implementation">GloVe Implementation</h3> <p>Here we will be using text corpus of cleaned wikipedia articles provided by Matt Mahoney.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">s3</span><span class="p">.</span><span class="n">amazonaws</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">video</span><span class="p">.</span><span class="n">udacity</span><span class="o">-</span><span class="n">data</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">topher</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="n">October</span><span class="o">/</span><span class="mi">5</span><span class="n">bbe6499_text8</span><span class="o">/</span><span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">text8</span><span class="p">.</span><span class="nb">zip</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">"</span><span class="s">retina</span><span class="sh">"</span>

<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check if gpu is available since training is faster
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GloVeDataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Prepares the training data for the glove model.
            Params:
                corpus (string): corpus of words
                min_count (int): words with minimum occurrence to consider
                window_size (int): context window size for generating co-occurrence matrix
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="c1"># only consider the words that occur more than 5 times in the corpus 
</span>        <span class="n">word_counts</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">({</span><span class="n">word</span><span class="p">:</span><span class="n">count</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">})</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="nf">most_common</span><span class="p">())}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

        <span class="c1"># create the training corpus
</span>        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">word2idx</span><span class="p">]</span>

        <span class="c1"># create the co-occurrence matrix for corpus
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">create_cooccurrence_matrix</span><span class="p">()</span>


    <span class="k">def</span> <span class="nf">create_cooccurrence_matrix</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the co-occurence matrix of center and context words based on the context window size.
        </span><span class="sh">"""</span>
        <span class="n">cooccurrence_counts</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="n">Counter</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">current_idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="c1"># find the start and end of context window
</span>            <span class="n">left_boundary</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">right_boundary</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">))</span>

            <span class="c1"># obtain the context words and center words based on context window
</span>            <span class="n">context_word_ids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">left_boundary</span><span class="p">:</span><span class="n">current_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">right_boundary</span><span class="p">]</span>
            <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">current_idx</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">context_word_id</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">context_word_ids</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">current_idx</span> <span class="o">!=</span> <span class="n">idx</span><span class="p">:</span>
                    <span class="c1"># add (1 / distance from center word) for this pair
</span>                    <span class="n">cooccurrence_counts</span><span class="p">[</span><span class="n">center_word_id</span><span class="p">][</span><span class="n">context_word_id</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nf">abs</span><span class="p">(</span><span class="n">current_idx</span> <span class="o">-</span> <span class="n">idx</span><span class="p">)</span>
        
        <span class="c1"># create tensors for input word ids, output word ids and their co-occurence count
</span>        <span class="n">in_ids</span><span class="p">,</span> <span class="n">out_ids</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">center_word_id</span><span class="p">,</span> <span class="n">counter</span> <span class="ow">in</span> <span class="n">cooccurrence_counts</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">context_word_id</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counter</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">in_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">center_word_id</span><span class="p">)</span>
                <span class="n">out_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">context_word_id</span><span class="p">)</span>
                <span class="n">counts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">in_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">in_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">out_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cooccurrence_counts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Creates the batches for training the network.
            Params:
                batch_size (int): size of the batch
            Returns:
                batch (torch tensor of shape (batch_size, 3)): tensor of word pair ids and 
                    co-occurence counts for a given batch
        </span><span class="sh">"""</span>
        <span class="n">random_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_ids</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_ids</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">random_ids</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">random_ids</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="k">yield</span> <span class="n">self</span><span class="p">.</span><span class="n">in_ids</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">out_ids</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">cooccurrence_counts</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read the file and initialize the GloVeDataset
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">text8</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nc">GloVeDataset</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GloVeModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">x_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> GloVe model for learning word embeddings. Uses the approach of predicting 
            context words given the center word.
            Params:
                vocab_size (int): number of words in the vocabulary
                embed_dim (int): embeddings of dimension to be generated
                x_max (int): cutoff of the weighting function
                alpha (int): parameter of the weighting funtion
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">GloVeModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_max</span> <span class="o">=</span> <span class="n">x_max</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

        <span class="c1"># embedding layers for input (center) and output (context) words along with biases
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_in</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># initialize the embeddings with uniform dist and set bias to zero
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>

    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_ids</span><span class="p">,</span> <span class="n">out_ids</span><span class="p">,</span> <span class="n">cooccurrence_counts</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Trains the GloVe model and updates the weights based on the
            criterion.
            Params:
                in_ids (torch tensor of shape (batch_size,)): indexes of the input words for a batch
                out_ids (torch tensor of shape (batch_size,)): indexes of the output words for a batch
                cooccurrence_counts (torch tensor of shape (batch_size,)): co-occurence count of input 
                    and output words for a batch
        </span><span class="sh">"""</span>
        <span class="n">emb_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_in</span><span class="p">(</span><span class="n">in_ids</span><span class="p">)</span>
        <span class="n">emb_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed_out</span><span class="p">(</span><span class="n">out_ids</span><span class="p">)</span>
        <span class="n">b_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bias_in</span><span class="p">(</span><span class="n">in_ids</span><span class="p">)</span>
        <span class="n">b_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bias_out</span><span class="p">(</span><span class="n">out_ids</span><span class="p">)</span>

        <span class="c1"># add 1 to counts i.e. cooccurrences in order to avoid log(0) case
</span>        <span class="n">cooccurrence_counts</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># count weight factor
</span>        <span class="n">weight_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">cooccurrence_counts</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">x_max</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">weight_factor</span><span class="p">[</span><span class="n">cooccurrence_counts</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="c1"># calculate the distance between the input and output embeddings
</span>        <span class="n">emb_prods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">emb_in</span> <span class="o">*</span> <span class="n">emb_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_cooccurrences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">cooccurrence_counts</span><span class="p">)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="p">(</span><span class="n">emb_prods</span> <span class="o">+</span> <span class="n">b_in</span> <span class="o">+</span> <span class="n">b_out</span> <span class="o">-</span> <span class="n">log_cooccurrences</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">weight_factor</span> <span class="o">*</span> <span class="n">distances</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># intialize the model and optimizer
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">GloVeModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adagrad</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training the network
</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Start of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">input_word_ids</span><span class="p">,</span> <span class="n">target_word_ids</span><span class="p">,</span> <span class="n">cooccurrence_counts</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">get_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># load tensor to GPU
</span>        <span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">input_word_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target_word_ids</span> <span class="o">=</span> <span class="n">target_word_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">cooccurrence_counts</span> <span class="o">=</span> <span class="n">cooccurrence_counts</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># forward pass
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">,</span> <span class="n">target_word_ids</span><span class="p">,</span> <span class="n">cooccurrence_counts</span><span class="p">)</span>

        <span class="c1"># backward pass, optimize
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epochs: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="se">\t</span><span class="s">Avg training loss: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="se">\t</span><span class="s">Ellapsed time: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> s</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">End of training</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the trained embeddings from the model
</span><span class="n">emb_in</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">embed_in</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">emb_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">embed_out</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">emb_in</span> <span class="o">+</span> <span class="n">emb_out</span>

<span class="c1"># number of words to be visualized
</span><span class="n">viz_words</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># projecting the embedding dimension from 300 to 2
</span><span class="n">tsne</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">()</span>
<span class="n">embed_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[:</span><span class="n">viz_words</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># plot the projected embeddings
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">viz_words</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">idx2word</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="p">(</span><span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embed_tsne</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div></div> <div class="citations"> <d-cite key="Mikolov2013EfficientEO"> <d-cite key="Mikolov2013DistributedRO"> <d-cite key="mccormick2016Word2Vec"> <d-cite key="mccormick2017Word2Vec"> <d-cite key="cs224n_word2vec"> <d-cite key="kaggle_word2vec"> <d-cite key="Pennington2014GloVeGV"> <d-cite key="group_homomorphism_wiki"> <d-cite key="glove_stack_exchange"> <d-cite key="gauthier_glove"> <d-cite key="gavrilov_glove"> &lt;/div&gt; </d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></d-cite> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Jay Gala. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: October 23, 2025. </div> </footer> <d-bibliography src="/assets/bibliography/2021-04-20-word_embeddings.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-BD3M6B4H2R"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-BD3M6B4H2R");</script> </body> </html>