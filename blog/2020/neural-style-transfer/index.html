<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Neural Style Transfer | Jay Gala</title> <meta name="author" content="Jay Gala"/> <meta name="description" content="This blog walks through the intuition behind the neural style transfer and its implementation."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap" rel="stylesheet"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaygala24.github.io/blog/2020/neural-style-transfer/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">d-article h3{margin-top:.5rem}@media(min-width:576px){.output-plot img{display:block;margin-left:auto;margin-right:auto;width:50%}}.citations{display:none}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Neural Style Transfer",
      "description": "This blog walks through the intuition behind the neural style transfer and its implementation.",
      "published": "July 31, 2020",
      "authors": [
        {
          "author": "Jay Gala",
          "authorURL": "https://jaygala24.github.io",
          "affiliations": [
            {
              "name": "University of Mumbai",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%61%79%67%61%6C%61%32%34@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=lNn2qGoAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/1992915388" title="Semantic Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jaygala24" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jaygala24" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/jaygala24" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/cv.pdf">cv</a> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Neural Style Transfer</h1> <p>This blog walks through the intuition behind the neural style transfer and its implementation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#intuition">Intuition</a></div> <div><a href="#losses-involved">Losses Involved</a></div> <div><a href="#code-walkthrough">Code Walkthrough</a></div> <div><a href="#future-resources">Future Resources</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <p>This post assumes that you have basic skills of working with <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch</a>. If you are new to PyTorch, I would highly encourage you to go through <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener noreferrer">Deep Leaning With PyTorch: A 60 Minute Blitz</a> by PyTorch. It’s a great place for beginners to get your hands dirty.</p> <p>Neural Style Transfer is an algorithm developed by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge that blends the content of one image with the style of another image using Deep Neural Networks to create artistic images of high perceptual quality.</p> <h2 id="intuition">Intuition</h2> <p>Convolutional Neural Networks are very powerful, extracting the visual information hierarchically. This makes them really useful for this task. The lower layers care more about the detailed pixel values, whereas the higher layers care more about the actual content of the image (objects such as eyes, nose, etc.).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neural_style_transfer/cnn_layer_output-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neural_style_transfer/cnn_layer_output-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neural_style_transfer/cnn_layer_output-1400.webp"></source> <img src="/assets/img/neural_style_transfer/cnn_layer_output.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Convolutional Neural Network (CNN) (<a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener noreferrer">image source</a>) </div> <p>In the above figure, the output image is a mix of two since we use the activations of the neural network at specific layers as a filter to get the intermediate style and content output of the inputs.</p> <p>The principle underlying the neural style transfer is simple:</p> <ul> <li>Define two distances, one for the content and one for style.</li> <li>Measure how different the content and style are between two images, respectively.</li> <li>Reconstruct the image from white noise using backpropagation by minimizing both content and style distance with the content and style images, respectively.</li> </ul> <h2 id="losses-involved">Losses Involved</h2> <h3 id="content-loss">Content Loss</h3> \[L_{content}(\bar{p}, \bar{x}, \bar{l}) = \frac{1}{2}\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2\] <p>where,</p> <ul> <li>$\bar{p}$ and $\bar{x}$ are the content and generated images respectively.</li> <li>$F_{i,j}$ and $P_{i,j}$ are the feature representation of the original and generated image of $i^{th}$ filter at position $j$ in layer $l$ respectively.</li> </ul> <h3 id="style-loss">Style Loss</h3> \[E_{l} = \frac{1}{4N_{l}^2M_{l}^2}\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2\] \[L_{style}(\bar{a}, \bar{x}) = \sum_{i=0}^{L}w_{l}E_{l}\] <p>where,</p> <ul> <li>$\bar{a}$ and $\bar{x}$ are the style and generated image respectively.</li> <li>$A^{l}$ and $G^{l}$ are the style representation (Gram Matrix) at layer $l$ respectively.</li> <li>$w_{l}$ and $E_{l}$ are weighing factor and error for specific layer ${l}$ respectively.</li> </ul> <h3 id="total-loss">Total Loss</h3> <p>which is a weighted sum of the two above:</p> \[L_{total}(\bar{p}, \bar{a}, \bar{x}) = \alpha L_{content}(\bar{p}, \bar{x}) + \beta L_{style}(\bar{a}, \bar{x})\] <p>where,</p> <ul> <li>$L_{content}$ and $L_{style}$ are the content and style loss respectively.</li> <li>$\alpha$ and $\beta$ are weights for the content and style loss, respectively.</li> <li>$\bar{p}$, $\bar{a}$ $\bar{x}$ are the content, style and generated images respectively.</li> </ul> <p>There is a trade-off between the actual content and artistic style, which is determined by $\alpha$ and $\beta$. If the content is more important, then increase the $\alpha$. If the style is more important, then increase the $\beta$.</p> <h2 id="code-walkthrough">Code Walkthrough</h2> <p>Now, let’s go to the implementation of the above algorithm by importing the below packages.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <p>Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using <code class="language-plaintext highlighter-rouge">torch.cuda.is_available()</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># device configuration
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Now, let’s download and load the pre-trained VGG19 model. VGG is trained for the task of object detection. We freeze all VGG parameters as we are using it for optimizing the target image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load the pretrained VGG19
</span><span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">features</span>

<span class="c1"># move the vgg model to GPU in eval mode (freeze model parameters) if available
</span><span class="n">vgg</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace=True)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace=True)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace=True)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace=True)
  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (17): ReLU(inplace=True)
  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace=True)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace=True)
  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (24): ReLU(inplace=True)
  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (26): ReLU(inplace=True)
  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace=True)
  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (31): ReLU(inplace=True)
  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (33): ReLU(inplace=True)
  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (35): ReLU(inplace=True)
  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
</code></pre></div></div> <p>Now, let’s load the style and content images. The PIL images loaded have values between 0 to 255, but when they are transformed into torch tensors, their values are converted between 0 and 1. We perform few transformations such as <code class="language-plaintext highlighter-rouge">Resize()</code>, <code class="language-plaintext highlighter-rouge">ToTensor()</code>, <code class="language-plaintext highlighter-rouge">Normalize()</code> on the image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># desired size of the output image
</span><span class="n">imsize</span> <span class="o">=</span> <span class="mi">512</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="mi">128</span>  <span class="c1"># use small size if no gpu
</span>
<span class="c1"># VGG19 mean and std for each channel
</span><span class="n">cnn_normalization_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
<span class="n">cnn_normalization_std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="c1"># scale imported image
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="n">imsize</span><span class="p">),</span>
    <span class="c1"># transform it into a torch tensor
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="c1"># normalize the tensor as per VGG network
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">cnn_normalization_mean</span><span class="p">,</span> 
                             <span class="n">std</span><span class="o">=</span><span class="n">cnn_normalization_std</span><span class="p">)</span>
<span class="p">])</span>

<span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Load an image and comvert it to a torch tensor.
    </span><span class="sh">"""</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">transform</span><span class="p">:</span>
        <span class="c1"># transform the image
</span>        <span class="n">image</span> <span class="o">=</span> <span class="nf">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="c1"># add a fake batch dimension to fit network's input dimension
</span>        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load the style and content image
</span><span class="n">style_img</span> <span class="o">=</span> <span class="nf">load_image</span><span class="p">(</span><span class="sh">'</span><span class="s">./images/style.jpg</span><span class="sh">'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">loader</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">content_img</span> <span class="o">=</span> <span class="nf">load_image</span><span class="p">(</span><span class="sh">'</span><span class="s">./images/content.jpg</span><span class="sh">'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">loader</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>Now, let’s create a function to denormalize the image tensors, which will be later helpful to display the image tensors.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">denorm_image</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Denormalize the image for visualization
    </span><span class="sh">"""</span>
    <span class="c1"># clone the image tensor and detach from tracking
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
    
    <span class="c1"># remove the fake batch dimension
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">()</span>   
    <span class="c1"># reshape (n_C, n_H, n_W) -&gt; (n_H, n_W, n_C)
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># denormalize the image
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">*</span> <span class="n">cnn_normalization_std</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">cnn_normalization_mean</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    
    <span class="c1"># restrict the value between 0 and 1 by clipping the outliers
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></div> <p>Now, let’s display the content and style image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">denorm_image</span><span class="p">(</span><span class="n">content_img</span><span class="p">))</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Content</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">denorm_image</span><span class="p">(</span><span class="n">style_img</span><span class="p">))</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Style</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neural_style_transfer/content_and_style_plot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neural_style_transfer/content_and_style_plot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neural_style_transfer/content_and_style_plot-1400.webp"></source> <img src="/assets/img/neural_style_transfer/content_and_style_plot.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Now, let’s select the convolutional layers from VGG19 to extract the feature maps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Extract the convolutional feature maps from conv1_1 ~ conv5_1
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># layer number for conv1_1 ~ conv5_1
</span>        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">10</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">19</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">28</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># conv feature map
</span>    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">image</span>
    
    <span class="c1"># iterate through the model layers
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">_modules</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># checks for the layer match
</span>        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">features</span>
</code></pre></div></div> <p>Now, let’s define the gram matrix used in style loss, which we try to minimize during the backpropagation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gram_matrix</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculates the gram matrix for input
    </span><span class="sh">"""</span>
    <span class="c1"># a = 1 (batch_size), b = n_C (number of feature maps)
</span>    <span class="c1"># (c, d) = dimension of feature map 
</span>    <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>

    <span class="c1"># reshape the convolutional feature maps
</span>    <span class="n">features</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>

    <span class="c1"># compute the gram matrix
</span>    <span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="p">.</span><span class="nf">t</span><span class="p">())</span>

    <span class="c1"># Normalize the values of gram matrix
</span>    <span class="n">G</span> <span class="o">=</span> <span class="n">G</span><span class="p">.</span><span class="nf">div</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">G</span>
</code></pre></div></div> <p>Now, let’s create a clone of the content image as a starting image for the target, which we transform such that the content image has an artistic style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_img</span> <span class="o">=</span> <span class="n">content_img</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Alternative way: you can start with white noise to get an image with 
# content attributes of content image and style attributes of style image
# target_img = torch.randn(content_img.data.size()).to(device)
</span></code></pre></div></div> <p>Now let’s run the model and try to minimize the loss using backpropagation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_neural_style_transfer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">content_img</span><span class="p">,</span> <span class="n">style_img</span><span class="p">,</span> <span class="n">target_img</span><span class="p">,</span> 
                              <span class="n">num_steps</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">sample_steps</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
                              <span class="n">style_weight</span><span class="o">=</span><span class="mf">1e4</span><span class="p">,</span> <span class="n">content_weight</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Run the neural style transfer
    </span><span class="sh">"""</span>
    <span class="c1"># optimizer for reconstruction of content image with artistic style
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">target_img</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">()],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> 
                                 <span class="n">betas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        
        <span class="c1"># extract the conv feature maps for target, content and style images
</span>        <span class="n">target_features</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">target_img</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">content_features</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">content_img</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">style_features</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">style_img</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    
        <span class="c1"># initialize the style and content loss
</span>        <span class="n">style_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">content_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># calculate the style and content loss for each specific layer
</span>        <span class="k">for</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">f3</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">target_features</span><span class="p">,</span> <span class="n">content_features</span><span class="p">,</span> <span class="n">style_features</span><span class="p">):</span>

            <span class="c1"># compute content loss with target and content images
</span>            <span class="n">content_loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

            <span class="c1"># compute the gram matrix for target and style feature maps
</span>            <span class="n">f1</span> <span class="o">=</span> <span class="nf">gram_matrix</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>
            <span class="n">f3</span> <span class="o">=</span> <span class="nf">gram_matrix</span><span class="p">(</span><span class="n">f3</span><span class="p">)</span>

            <span class="c1"># compute the style loss with target and style images
</span>            <span class="n">style_loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f3</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># compute total loss, backprop and optimize
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">content_loss</span> <span class="o">*</span> <span class="n">content_weight</span> <span class="o">+</span> <span class="n">style_loss</span> <span class="o">*</span> <span class="n">style_weight</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
        <span class="nf">if </span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">sample_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print the model stats
</span>            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">run {}:</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Style Loss : {:4f} Content Loss: {:4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                    <span class="n">style_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">content_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>
            <span class="nf">print</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">target_img</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># run the neural style transfer
</span><span class="n">output_img</span> <span class="o">=</span> <span class="nf">run_neural_style_transfer</span><span class="p">(</span><span class="n">vgg</span><span class="p">,</span> <span class="n">content_img</span><span class="p">,</span> <span class="n">style_img</span><span class="p">,</span> <span class="n">target_img</span><span class="p">)</span>

<span class="c1"># display the style transfered image
</span><span class="n">output_img</span> <span class="o">=</span> <span class="nf">denorm_image</span><span class="p">(</span><span class="n">output_img</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">output_img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Output Image</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>run 400:
Style Loss : 0.000006 Content Loss: 21.730204

run 800:
Style Loss : 0.000004 Content Loss: 19.803923

run 1200:
Style Loss : 0.000004 Content Loss: 19.264122

run 1600:
Style Loss : 0.000004 Content Loss: 19.008064

run 2000:
Style Loss : 0.000004 Content Loss: 18.843626
</code></pre></div></div> <div class="output-plot"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neural_style_transfer/inference-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neural_style_transfer/inference-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neural_style_transfer/inference-1400.webp"></source> <img src="/assets/img/neural_style_transfer/inference.jpeg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Great! Now you have become an artist who can generate artworks from a content image and style image.</p> <h2 id="future-resources">Future Resources</h2> <ul> <li>Try the <a href="https://github.com/pytorch/examples/tree/master/fast_neural_style" target="_blank" rel="noopener noreferrer">Fast Neural Style Transfer</a>, which uses <a href="https://arxiv.org/abs/1603.08155" target="_blank" rel="noopener noreferrer">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a> along with <a href="https://arxiv.org/pdf/1607.08022.pdf" target="_blank" rel="noopener noreferrer">Instance Normalization</a>.</li> <li>Try the <a href="https://www.tensorflow.org/tutorials/generative/style_transfer" target="_blank" rel="noopener noreferrer">Tensorflow Implementation</a> of Neural Style Transfer.</li> </ul> <div class="citations"> <d-cite key="gatys_2015_style_transfer"> <d-cite key="jacq_2021_style_transfer_pytorch"> &lt;/div&gt; </d-cite></d-cite> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Jay Gala. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: October 23, 2025. </div> </footer> <d-bibliography src="/assets/bibliography/2020-07-31-neural-style-transfer.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-BD3M6B4H2R"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-BD3M6B4H2R");</script> </body> </html>